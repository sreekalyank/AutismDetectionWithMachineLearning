{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301cf963-29f9-4579-af86-716bfe76862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95833333 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy: 0.9958333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation: 0.9807692307692307\n",
      "printing precision\n",
      "0.9827586206896552\n",
      "f1-score\n",
      "0.9805897723030981\n",
      "Testing Set Accuracy without cross-validation: 0.9807692307692307\n",
      "Testing Set Accuracy without cross-validation: 0.9807692307692307\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9791666666666667\n",
      "kappa score\n",
      "0.9611940298507463\n",
      "log loss\n",
      "0.07685805066030249\n",
      "MCC\n",
      "0.9619185809213996\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.175, random_state=35)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=32, learning_rate=0.1, algorithm='SAMME.R', random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=10)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_qt_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f38e658-2e60-4def-800d-4fc36cdda357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree): [0.95454545 1.         0.90909091 0.95454545 1.         0.95454545\n",
      " 0.95238095 0.95238095 1.         0.9047619 ]\n",
      "Mean CV accuracy (Decision Tree): 0.9582251082251083\n",
      "Testing Set Accuracy with cross-validation (Decision Tree): 0.9605263157894737\n",
      "printing precision\n",
      "0.9651162790697674\n",
      "f1-score\n",
      "0.9601885804085909\n",
      "Testing Set Accuracy without cross-validation: 0.9605263157894737\n",
      "Testing Set Accuracy without cross-validation: 0.9605263157894737\n",
      "ROC AUC: 0.9583333333333333\n",
      "recall\n",
      "0.9583333333333333\n",
      "kappa score\n",
      "0.9205020920502092\n",
      "log loss\n",
      "1.4227757916756774\n",
      "MCC\n",
      "0.9234247009337435\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.26, random_state=33)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=None, min_samples_split=4,random_state=39)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_dt = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree):\", cv_scores_dt)\n",
    "print(\"Mean CV accuracy (Decision Tree):\", np.mean(cv_scores_dt))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_dt\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_qt_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3f30ba-68d3-4886-b5aa-a1275ca4f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (189). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (GNB with QuantileTransformer): [1.         1.         1.         0.95238095 1.         1.\n",
      " 1.         1.         1.         0.95238095]\n",
      "Mean CV accuracy (GNB with QuantileTransformer): 0.9904761904761905\n",
      "Testing Set Accuracy with cross-validation (GNB with QuantileTransformer): 0.9634146341463414\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9516129032258065\n",
      "F1-score: 0.9602905569007263\n",
      "ROC AUC: 0.9722222222222222\n",
      "Recall: 0.9722222222222222\n",
      "Kappa Score: 0.9206963249516441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (210). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 1.3011188599225232\n",
      "Matthews Correlation Coefficient: 0.9236052159781682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.28, random_state=31)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', QuantileTransformer())  # Use QuantileTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and GNB classifier for training and testing\n",
    "pipeline_gnb_quantile = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_gnb_quantile, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (GNB with QuantileTransformer):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (GNB with QuantileTransformer):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_gnb_quantile.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_gnb = pipeline_gnb_quantile.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test_gnb)\n",
    "print(\"Testing Set Accuracy with cross-validation (GNB with QuantileTransformer):\", test_accuracy_test_gnb)\n",
    "accuracy_qt_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_gnb = pipeline_gnb_quantile.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_gnb))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_gnb))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_gnb_quantile.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876eee50-fb1f-4b34-b9c0-e1c650d8442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [1.         1.         0.95833333 0.86956522 0.91304348 0.91304348\n",
      " 0.7826087  0.91304348 0.82608696 0.91304348]\n",
      "Mean CV accuracy (KNN): 0.9088768115942027\n",
      "Testing Set Accuracy with cross-validation (KNN): 0.9661016949152542\n",
      "printing precision\n",
      "0.9545454545454546\n",
      "f1-score\n",
      "0.963032581453634\n",
      "Testing Set Accuracy without cross-validation: 0.9661016949152542\n",
      "Testing Set Accuracy without cross-validation: 0.9661016949152542\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9743589743589743\n",
      "kappa score\n",
      "0.9261576971214017\n",
      "log loss\n",
      "0.2382090144922613\n",
      "MCC\n",
      "0.928693093799487\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance',algorithm='auto', p=3, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',]),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_cv_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_knn = cross_val_score(pipeline_cv_knn, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV accuracy (KNN):\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_knn\n",
    "pipeline_cv_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test_knn = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN):\", test_accuracy_test_knn)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_qt_knn=test_accuracy_test\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_knn.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4334dd-6dc7-4098-8416-39b5359a6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (38). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (38). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (38). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LDA with QuantileTransformer): [0.6  0.8  0.4  0.5  1.   1.   0.   0.75 1.   1.  ]\n",
      "Mean CV accuracy (LDA with QuantileTransformer): 0.705\n",
      "Testing Set Accuracy with cross-validation (LDA with QuantileTransformer): 0.5180722891566265\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.5236737400530505\n",
      "F1-score: 0.5167550782766206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (43). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.5371989121989122\n",
      "Recall: 0.5231157731157731\n",
      "Kappa Score: 0.04561134534304323\n",
      "Log Loss: 12.116874703575464\n",
      "Matthews Correlation Coefficient: 0.046786186160793386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer  # Replaced Normalizer with QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Import Linear Discriminant Analysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=38)\n",
    "\n",
    "# Define Linear Discriminant Analysis classifier\n",
    "lda = LinearDiscriminantAnalysis()  # Instantiate Linear Discriminant Analysis\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', QuantileTransformer())  # Use QuantileTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training and testing\n",
    "pipeline_lda_qt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lda)  # Use LDA classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lda = cross_val_score(pipeline_lda_qt, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LDA with QuantileTransformer):\", cv_scores_lda)\n",
    "print(\"Mean CV accuracy (LDA with QuantileTransformer):\", np.mean(cv_scores_lda))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lda_qt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lda = pipeline_lda_qt.predict(X_test)\n",
    "test_accuracy_test_lda = accuracy_score(y_test, y_pred_test_lda)\n",
    "print(\"Testing Set Accuracy with cross-validation (LDA with QuantileTransformer):\", test_accuracy_test_lda)\n",
    "accuracy_qt_lda=test_accuracy_test_lda\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lda, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lda = pipeline_lda_qt.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lda))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lda))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lda_qt.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3683169b-18e9-40ab-aba0-9a3b6260573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (26). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (27). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LR with QuantileTransformer): [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      "Mean CV accuracy (LR with QuantileTransformer): 0.9\n",
      "Testing Set Accuracy with cross-validation (LR with QuantileTransformer): 0.9581749049429658\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9595588235294117\n",
      "F1-score: 0.958172486084002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (29). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9969855072463768\n",
      "Recall: 0.9601449275362319\n",
      "Kappa Score: 0.9164910660162227\n",
      "Log Loss: 0.24557518410765228\n",
      "Matthews Correlation Coefficient: 0.9197035643109733\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=37)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()  # Instantiate Logistic Regression\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', QuantileTransformer())  # Use QuantileTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LR classifier for training and testing\n",
    "pipeline_lr_quantile = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)  # Use LR classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_lr_quantile, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LR with QuantileTransformer):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (LR with QuantileTransformer):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lr_quantile.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lr = pipeline_lr_quantile.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(\"Testing Set Accuracy with cross-validation (LR with QuantileTransformer):\", test_accuracy_test_lr)\n",
    "accuracy_qt_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lr, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lr = pipeline_lr_quantile.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lr))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lr))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lr_quantile.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f31ae2-51e5-4076-addd-4715ffbd2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Random Forest): [1.         1.         0.95833333 0.91304348 1.         0.86956522\n",
      " 1.         0.95652174 0.95652174 1.        ]\n",
      "Mean CV accuracy (Random Forest): 0.9653985507246376\n",
      "Testing Set Accuracy (Random Forest): 0.9661016949152542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# # Apply random oversampling\n",
    "# ros = RandomOverSampler(random_state=37)\n",
    "# X_resampled, y_resampled = ros.fit_resample(result_df, labels)\n",
    "\n",
    "# # Convert oversampled data into DataFrame\n",
    "# oversampled_df = pd.DataFrame(X_resampled, columns=result_df.columns)\n",
    "# oversampled_df = pd.concat([oversampled_df, pd.Series(y_resampled, name='target')], axis=1)\n",
    "\n",
    "\n",
    "# # Split oversampled dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(oversampled_df.drop('target', axis=1), oversampled_df['target'], test_size=0.44, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df,labels,test_size=0.2, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=700, max_depth=15, max_features=None, min_samples_leaf=4, warm_start=True, n_jobs=-1, oob_score=True, random_state=43)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_rf = cross_val_score(pipeline_rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Random Forest):\", cv_scores_rf)\n",
    "print(\"Mean CV accuracy (Random Forest):\", np.mean(cv_scores_rf))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_rf.predict(X_test)\n",
    "test_accuracy_test_rf = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy (Random Forest):\", test_accuracy_test_rf)\n",
    "accuracy_qt_rf=test_accuracy_test_rf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0655df40-2b5c-4638-9411-324eda155527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (38). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (38). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (38). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (39). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (SVM with QuantileTransformer): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy (SVM with QuantileTransformer): 1.0\n",
      "Testing Set Accuracy with cross-validation (SVM with QuantileTransformer): 0.963855421686747\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9633720930232558\n",
      "F1-score: 0.9637712789175032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (43). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9954027454027453\n",
      "Recall: 0.9644522144522145\n",
      "Kappa Score: 0.9275530986325283\n",
      "Log Loss: 0.10362394094386797\n",
      "Matthews Correlation Coefficient: 0.9278236787666256\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer  # Import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=38)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC(probability=True)  \n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', QuantileTransformer())  # Use QuantileTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training and testing\n",
    "pipeline_svm_qt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_svm_qt, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (SVM with QuantileTransformer):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (SVM with QuantileTransformer):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_svm_qt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_svm = pipeline_svm_qt.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(\"Testing Set Accuracy with cross-validation (SVM with QuantileTransformer):\", test_accuracy_test_svm)\n",
    "accuracy_qt_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_svm, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_svm = pipeline_svm_qt.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_svm))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_svm))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_svm_qt.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8998186d-460d-4a83-9d7a-a3efec651574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         1.         0.95454545 1.         1.         0.95454545\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy: 0.990909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation: 0.9726027397260274\n",
      "printing precision\n",
      "0.9736842105263157\n",
      "f1-score\n",
      "0.9725975975975976\n",
      "Testing Set Accuracy without cross-validation: 0.9726027397260274\n",
      "Testing Set Accuracy without cross-validation: 0.9726027397260274\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.972972972972973\n",
      "kappa score\n",
      "0.9452363090772693\n",
      "log loss\n",
      "0.09063688187584\n",
      "MCC\n",
      "0.9466569163176007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=35)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=32, learning_rate=0.1, algorithm='SAMME.R', random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e01a673-6081-4eb9-9296-8f88ff7a0e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree): [1.         1.         1.         1.         1.         0.95454545\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy (Decision Tree): 0.9954545454545455\n",
      "Testing Set Accuracy with cross-validation (Decision Tree): 0.9726027397260274\n",
      "printing precision\n",
      "0.9795918367346939\n",
      "f1-score\n",
      "0.9695833333333334\n",
      "Testing Set Accuracy without cross-validation: 0.9726027397260274\n",
      "Testing Set Accuracy without cross-validation: 0.9726027397260274\n",
      "ROC AUC: 0.9615384615384616\n",
      "recall\n",
      "0.9615384615384616\n",
      "kappa score\n",
      "0.9392173189009159\n",
      "log loss\n",
      "0.9874973531264976\n",
      "MCC\n",
      "0.9409571265322042\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=41)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini',splitter='random',max_depth=None, min_samples_split=3,random_state=39)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))  # Using PowerTransformer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_dt = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree):\", cv_scores_dt)\n",
    "print(\"Mean CV accuracy (Decision Tree):\", np.mean(cv_scores_dt))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_dt\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111c5119-e130-4c10-ae15-d9b2987aa80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (GNB with PowerTransformer): [1.         1.         1.         0.95238095 1.         1.\n",
      " 1.         1.         1.         0.95238095]\n",
      "Mean CV accuracy (GNB with PowerTransformer): 0.9904761904761905\n",
      "Testing Set Accuracy with cross-validation (GNB with PowerTransformer): 0.9634146341463414\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9516129032258065\n",
      "F1-score: 0.9602905569007263\n",
      "ROC AUC: 0.9722222222222222\n",
      "Recall: 0.9722222222222222\n",
      "Kappa Score: 0.9206963249516441\n",
      "Log Loss: 1.2332774551007117\n",
      "Matthews Correlation Coefficient: 0.9236052159781682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.28, random_state=31)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', PowerTransformer())  # Use PowerTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and GNB classifier for training and testing\n",
    "pipeline_gnb_power = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_gnb_power, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (GNB with PowerTransformer):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (GNB with PowerTransformer):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_gnb_power.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_gnb = pipeline_gnb_power.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test_gnb)\n",
    "print(\"Testing Set Accuracy with cross-validation (GNB with PowerTransformer):\", test_accuracy_test_gnb)\n",
    "accuracy_pt_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_gnb = pipeline_gnb_power.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_gnb))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_gnb))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_gnb_power.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcab050d-0d4b-48b4-9601-698b6c09e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [1.         0.95454545 0.90909091 0.86363636 0.9047619  0.80952381\n",
      " 0.9047619  0.95238095 0.95238095 1.        ]\n",
      "Mean CV accuracy (KNN): 0.9251082251082252\n",
      "Testing Set Accuracy with cross-validation (KNN): 0.9615384615384616\n",
      "printing precision\n",
      "0.9611842105263158\n",
      "f1-score\n",
      "0.9614814814814814\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "ROC AUC: 0.991430454845089\n",
      "recall\n",
      "0.9620962425840475\n",
      "kappa score\n",
      "0.9229756418696511\n",
      "log loss\n",
      "0.15271752442696562\n",
      "MCC\n",
      "0.9232800026499022\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.265, random_state=44)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance', algorithm='auto', p=3, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))  # Using PowerTransformer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_cv_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_knn = cross_val_score(pipeline_cv_knn, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV accuracy (KNN):\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_knn\n",
    "pipeline_cv_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test_knn = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN):\", test_accuracy_test_knn)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_knn=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_knn.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7789de7e-3a3a-4b3a-a05b-7bd03c972636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LDA with PowerTransformer): [0.6  0.8  0.4  0.5  1.   1.   0.   0.75 1.   1.  ]\n",
      "Mean CV accuracy (LDA with PowerTransformer): 0.705\n",
      "Testing Set Accuracy with cross-validation (LDA with PowerTransformer): 0.5180722891566265\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.5236737400530505\n",
      "F1-score: 0.5167550782766206\n",
      "ROC AUC: 0.5371989121989122\n",
      "Recall: 0.5231157731157731\n",
      "Kappa Score: 0.04561134534304323\n",
      "Log Loss: 12.116874703575483\n",
      "Matthews Correlation Coefficient: 0.046786186160793386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer  # Replaced Normalizer with PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Import Linear Discriminant Analysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=38)\n",
    "\n",
    "# Define Linear Discriminant Analysis classifier\n",
    "lda = LinearDiscriminantAnalysis()  # Instantiate Linear Discriminant Analysis\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', PowerTransformer())  # Use PowerTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training and testing\n",
    "pipeline_lda_pt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lda)  # Use LDA classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lda = cross_val_score(pipeline_lda_pt, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LDA with PowerTransformer):\", cv_scores_lda)\n",
    "print(\"Mean CV accuracy (LDA with PowerTransformer):\", np.mean(cv_scores_lda))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lda_pt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lda = pipeline_lda_pt.predict(X_test)\n",
    "test_accuracy_test_lda = accuracy_score(y_test, y_pred_test_lda)\n",
    "print(\"Testing Set Accuracy with cross-validation (LDA with PowerTransformer):\", test_accuracy_test_lda)\n",
    "accuracy_pt_lda=test_accuracy_test_lda\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lda, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lda = pipeline_lda_pt.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lda))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lda))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lda_pt.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f7b7ff-bd97-4444-ab7c-80926ba49090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LR with PowerTransformer): [1.         1.         1.         1.         1.         0.66666667\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy (LR with PowerTransformer): 0.9666666666666666\n",
      "Testing Set Accuracy with cross-validation (LR with PowerTransformer): 0.9429657794676806\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9475524475524475\n",
      "F1-score: 0.9429129585413502\n",
      "ROC AUC: 0.9998842592592593\n",
      "Recall: 0.9444444444444444\n",
      "Kappa Score: 0.8861964517524881\n",
      "Log Loss: 0.17721984918434527\n",
      "Matthews Correlation Coefficient: 0.8919914773408968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=40)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()  # Instantiate Logistic Regression\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', PowerTransformer())  # Use PowerTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LR classifier for training and testing\n",
    "pipeline_lr_power = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)  # Use LR classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_lr_power, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LR with PowerTransformer):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (LR with PowerTransformer):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lr_power.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lr = pipeline_lr_power.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(\"Testing Set Accuracy with cross-validation (LR with PowerTransformer):\", test_accuracy_test_lr)\n",
    "accuracy_pt_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lr, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lr = pipeline_lr_power.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lr))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lr))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lr_power.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5c4faeb-314f-4ced-8d21-ab84192d8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Random Forest): [0.9047619 1.        1.        1.        1.        0.95      0.9\n",
      " 1.        0.95      1.       ]\n",
      "Mean CV accuracy (Random Forest): 0.9704761904761904\n",
      "Testing Set Accuracy (Random Forest): 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.3, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=700, max_depth=15, max_features=None, min_samples_leaf=4, warm_start=True, n_jobs=-1, oob_score=True, random_state=43)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson'))  # Use PowerTransformer instead of QuantileTransformer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_rf = cross_val_score(pipeline_rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Random Forest):\", cv_scores_rf)\n",
    "print(\"Mean CV accuracy (Random Forest):\", np.mean(cv_scores_rf))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_rf.predict(X_test)\n",
    "test_accuracy_test_rf = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy (Random Forest):\", test_accuracy_test_rf)\n",
    "accuracy_pt_rf=test_accuracy_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "381258e9-3dda-4dde-9285-16b85356d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (SVM with PowerTransformer): [1.  0.4 0.8 1.  1.  1.  1.  1.  1.  1. ]\n",
      "Mean CV accuracy (SVM with PowerTransformer): 0.9199999999999999\n",
      "Testing Set Accuracy with cross-validation (SVM with PowerTransformer): 0.9558232931726908\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9573643410852712\n",
      "F1-score: 0.9558204429246577\n",
      "ROC AUC: 1.0\n",
      "Recall: 0.9580152671755725\n",
      "Kappa Score: 0.911813001062494\n",
      "Log Loss: 0.09563258394159518\n",
      "Matthews Correlation Coefficient: 0.9153793768241673\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer  # Import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=45)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC(probability=True)  \n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', PowerTransformer())  # Use PowerTransformer instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training and testing\n",
    "pipeline_svm_pt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_svm_pt, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (SVM with PowerTransformer):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (SVM with PowerTransformer):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_svm_pt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_svm = pipeline_svm_pt.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(\"Testing Set Accuracy with cross-validation (SVM with PowerTransformer):\", test_accuracy_test_svm)\n",
    "accuracy_pt_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_svm, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_svm = pipeline_svm_pt.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_svm))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_svm))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_svm_pt.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee0031d-8caf-4d7a-a4bc-df2e4277ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.   1.   1.   1.   0.95 1.   1.   1.   0.95 1.  ]\n",
      "Mean CV accuracy: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation: 0.9772727272727273\n",
      "printing precision\n",
      "0.9821428571428572\n",
      "f1-score\n",
      "0.9757575757575758\n",
      "Testing Set Accuracy without cross-validation: 0.9772727272727273\n",
      "Testing Set Accuracy without cross-validation: 0.9772727272727273\n",
      "ROC AUC: 0.9733115468409586\n",
      "recall\n",
      "0.9705882352941176\n",
      "kappa score\n",
      "0.9515418502202643\n",
      "log loss\n",
      "0.19739487111688303\n",
      "MCC\n",
      "0.9526610232449336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.3, random_state=41)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=32, learning_rate=0.1, algorithm='SAMME.R', random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Using Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aabebf6-d6be-4057-83d6-b706a650febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree): [1.         1.         1.         0.95238095 1.         0.95\n",
      " 0.95       1.         1.         1.        ]\n",
      "Mean CV accuracy (Decision Tree): 0.9852380952380952\n",
      "Testing Set Accuracy with cross-validation (Decision Tree): 0.9545454545454546\n",
      "printing precision\n",
      "0.9550387596899225\n",
      "f1-score\n",
      "0.9545454545454545\n",
      "Testing Set Accuracy without cross-validation: 0.9545454545454546\n",
      "Testing Set Accuracy without cross-validation: 0.9545454545454546\n",
      "ROC AUC: 0.9550387596899225\n",
      "recall\n",
      "0.9550387596899225\n",
      "kappa score\n",
      "0.909137842023748\n",
      "log loss\n",
      "1.638347881323507\n",
      "MCC\n",
      "0.9100775193798449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.3, random_state=38)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini',splitter='random',max_depth=None, min_samples_split=3,random_state=39)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Using Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_dt = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree):\", cv_scores_dt)\n",
    "print(\"Mean CV accuracy (Decision Tree):\", np.mean(cv_scores_dt))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_dt\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a17d7b-393f-4d46-92af-c90bd2394445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (GNB with Normalizer): [1.         1.         1.         1.         1.         0.95454545\n",
      " 1.         0.95238095 1.         1.        ]\n",
      "Mean CV accuracy (GNB with Normalizer): 0.9906926406926406\n",
      "Testing Set Accuracy with cross-validation (GNB with Normalizer): 0.9605263157894737\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9464285714285714\n",
      "F1-score: 0.9565465980560321\n",
      "ROC AUC: 0.9705882352941176\n",
      "Recall: 0.9705882352941176\n",
      "Kappa Score: 0.91324200913242\n",
      "Log Loss: 1.4052638856021205\n",
      "Matthews Correlation Coefficient: 0.9166984970282113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.26, random_state=31)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())  # Use Normalizer instead of MaxAbsScaler\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and GNB classifier for training and testing\n",
    "pipeline_gnb_normalizer = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_gnb_normalizer, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (GNB with Normalizer):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (GNB with Normalizer):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_gnb_normalizer.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_gnb = pipeline_gnb_normalizer.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test_gnb)\n",
    "print(\"Testing Set Accuracy with cross-validation (GNB with Normalizer):\", test_accuracy_test_gnb)\n",
    "accuracy_nor_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_gnb = pipeline_gnb_normalizer.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_gnb))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_gnb))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_gnb_normalizer.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c8e5f8e-194b-4161-8c4d-e9eaa8254fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [0.86956522 0.91304348 0.95652174 0.7826087  0.7826087  0.82608696\n",
      " 0.82608696 0.90909091 0.81818182 0.81818182]\n",
      "Mean CV accuracy (KNN): 0.850197628458498\n",
      "Testing Set Accuracy with cross-validation (KNN): 0.9538461538461539\n",
      "printing precision\n",
      "0.9487179487179487\n",
      "f1-score\n",
      "0.9516008935219658\n",
      "Testing Set Accuracy without cross-validation: 0.9538461538461539\n",
      "Testing Set Accuracy without cross-validation: 0.9538461538461539\n",
      "ROC AUC: 0.992\n",
      "recall\n",
      "0.955\n",
      "kappa score\n",
      "0.9032258064516129\n",
      "log loss\n",
      "0.2088360437635039\n",
      "MCC\n",
      "0.9036961141150639\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=32)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance', algorithm='auto', p=3, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Using Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_cv_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_knn = cross_val_score(pipeline_cv_knn, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV accuracy (KNN):\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_knn\n",
    "pipeline_cv_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test= pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test_knn = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN):\", test_accuracy_test_knn)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_knn=test_accuracy_test\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_knn.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfe4155b-8fe2-4973-a4df-9635e2a8ec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LDA with Normalizer): [0.6  0.8  0.4  0.5  0.75 0.75 0.   0.5  1.   0.75]\n",
      "Mean CV accuracy (LDA with Normalizer): 0.605\n",
      "Testing Set Accuracy with cross-validation (LDA with Normalizer): 0.5020080321285141\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.507703081232493\n",
      "F1-score: 0.5001942501942502\n",
      "ROC AUC: 0.5328282828282829\n",
      "Recall: 0.5074786324786325\n",
      "Kappa Score: 0.014742485161784336\n",
      "Log Loss: 13.513213405727608\n",
      "Matthews Correlation Coefficient: 0.015180054478277269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer  # Replaced MaxAbsScaler with Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Import Linear Discriminant Analysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=38)\n",
    "\n",
    "# Define Linear Discriminant Analysis classifier\n",
    "lda = LinearDiscriminantAnalysis()  # Instantiate Linear Discriminant Analysis\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())  # Use Normalizer instead of MaxAbsScaler\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training and testing\n",
    "pipeline_lda_nor = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lda)  # Use LDA classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lda = cross_val_score(pipeline_lda_nor, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LDA with Normalizer):\", cv_scores_lda)\n",
    "print(\"Mean CV accuracy (LDA with Normalizer):\", np.mean(cv_scores_lda))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lda_nor.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lda = pipeline_lda_nor.predict(X_test)\n",
    "test_accuracy_test_lda = accuracy_score(y_test, y_pred_test_lda)\n",
    "print(\"Testing Set Accuracy with cross-validation (LDA with Normalizer):\", test_accuracy_test_lda)\n",
    "accuracy_nor_lda=test_accuracy_test_lda\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lda, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lda = pipeline_lda_nor.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lda))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lda))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lda_nor.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9d200-cdfa-480e-a612-b66b5e84836f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a45add98-8fba-4f03-bae7-a35de11b3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LR with Normalizer): [1.         1.         1.         0.66666667 1.         1.\n",
      " 0.83333333 1.         1.         1.        ]\n",
      "Mean CV accuracy (LR with Normalizer): 0.95\n",
      "Testing Set Accuracy with cross-validation (LR with Normalizer): 0.9401709401709402\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9403857393337229\n",
      "F1-score: 0.940153452685422\n",
      "ROC AUC: 0.9918032786885246\n",
      "Recall: 0.9411592505854801\n",
      "Kappa Score: 0.8803855703227691\n",
      "Log Loss: 0.25441041828744226\n",
      "Matthews Correlation Coefficient: 0.8815446505605892\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.8, random_state=38)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()  # Instantiate Logistic Regression\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())  # Use Normalizer instead of MaxAbsScaler\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LR classifier for training and testing\n",
    "pipeline_lr_normalizer = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)  # Use LR classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_lr_normalizer, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LR with Normalizer):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (LR with Normalizer):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lr_normalizer.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lr = pipeline_lr_normalizer.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(\"Testing Set Accuracy with cross-validation (LR with Normalizer):\", test_accuracy_test_lr)\n",
    "accuracy_nor_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lr, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lr = pipeline_lr_normalizer.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lr))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lr))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lr_normalizer.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a11db15e-7f7f-49cb-82ef-66dd23b1f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Random Forest): [0.94736842 0.94736842 1.         0.94736842 0.89473684 1.\n",
      " 1.         1.         0.88888889 1.        ]\n",
      "Mean CV accuracy (Random Forest): 0.9625730994152046\n",
      "Testing Set Accuracy (Random Forest): 0.9433962264150944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.36, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=700, max_depth=15, max_features=None, min_samples_leaf=4, warm_start=True, n_jobs=-1, oob_score=True, random_state=43)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Use Normalizer instead of PowerTransformer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_rf = cross_val_score(pipeline_rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Random Forest):\", cv_scores_rf)\n",
    "print(\"Mean CV accuracy (Random Forest):\", np.mean(cv_scores_rf))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_rf.predict(X_test)\n",
    "test_accuracy_test_rf = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy (Random Forest):\", test_accuracy_test_rf)\n",
    "accuracy_nor_rf=test_accuracy_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec16d9c6-4fa5-4c31-b982-989773486cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (SVM with Normalizer): [1.   0.4  0.6  1.   1.   0.75 0.5  1.   0.75 0.75]\n",
      "Mean CV accuracy (SVM with Normalizer): 0.775\n",
      "Testing Set Accuracy with cross-validation (SVM with Normalizer): 0.9437751004016064\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9436774193548387\n",
      "F1-score: 0.9437306301652892\n",
      "ROC AUC: 0.9855091214904903\n",
      "Recall: 0.9448829085263294\n",
      "Kappa Score: 0.8875266180551075\n",
      "Log Loss: 0.589321476101115\n",
      "Matthews Correlation Coefficient: 0.8885595101512034\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer  # Import Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=45)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC(probability=True)  \n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())  # Use Normalizer instead of MaxAbsScaler\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training and testing\n",
    "pipeline_svm_nor = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_svm_nor, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (SVM with Normalizer):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (SVM with Normalizer):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_svm_nor.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_svm = pipeline_svm_nor.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(\"Testing Set Accuracy with cross-validation (SVM with Normalizer):\", test_accuracy_test_svm)\n",
    "accuracy_nor_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_svm, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_svm = pipeline_svm_nor.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_svm))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_svm))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_svm_nor.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "416a5670-f3a4-4b33-ae78-20c86eaab81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (AdaBoost with MaxAbsScaler): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy (AdaBoost with MaxAbsScaler): 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation (AdaBoost with MaxAbsScaler): 0.9736842105263158\n",
      "printing precision\n",
      "0.975609756097561\n",
      "f1-score\n",
      "0.9736111111111111\n",
      "Testing Set Accuracy without cross-validation: 0.9736842105263158\n",
      "Testing Set Accuracy without cross-validation: 0.9736842105263158\n",
      "ROC AUC: 0.9778239778239779\n",
      "recall\n",
      "0.972972972972973\n",
      "kappa score\n",
      "0.9472588480222068\n",
      "log loss\n",
      "0.1318063677062129\n",
      "MCC\n",
      "0.9485790643197097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.26, random_state=35)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=100, learning_rate=0.1, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training and testing\n",
    "pipeline_cv_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (AdaBoost with MaxAbsScaler):\", cv_scores)\n",
    "print(\"Mean CV accuracy (AdaBoost with MaxAbsScaler):\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_mas.predict(X_test)\n",
    "test_accuracy_test_mas = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (AdaBoost with MaxAbsScaler):\", test_accuracy_test_mas)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_ab=test_accuracy_test\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_mas.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b78f1d-3c60-4270-98dc-617be93f9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree with MaxAbsScaler): [0.78947368 0.84210526 0.73684211 0.78947368 0.84210526 0.73684211\n",
      " 0.68421053 0.73684211 0.78947368 0.83333333]\n",
      "Mean CV accuracy (Decision Tree with MaxAbsScaler): 0.7780701754385965\n",
      "Testing Set Accuracy with cross-validation (Decision Tree with MaxAbsScaler): 0.9320388349514563\n",
      "printing precision\n",
      "0.9315610859728507\n",
      "f1-score\n",
      "0.9318074340300766\n",
      "Testing Set Accuracy without cross-validation: 0.9320388349514563\n",
      "Testing Set Accuracy without cross-validation: 0.9320388349514563\n",
      "ROC AUC: 0.9441266209000763\n",
      "recall\n",
      "0.9364988558352403\n",
      "kappa score\n",
      "0.8639365918097754\n",
      "log loss\n",
      "0.2412772963361468\n",
      "MCC\n",
      "0.8680458979804307\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.35, random_state=47)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=2, min_samples_split=2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree with MaxAbsScaler):\", cv_scores)\n",
    "print(\"Mean CV accuracy (Decision Tree with MaxAbsScaler):\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree with MaxAbsScaler):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_dt=test_accuracy_test\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86d81351-cef1-4db6-b759-b92a4a23f42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (GNB with MaxAbsScaler): [0.95652174 0.95652174 1.         1.         1.         1.\n",
      " 0.86956522 0.95454545 1.         1.        ]\n",
      "Mean CV accuracy (GNB with MaxAbsScaler): 0.9737154150197629\n",
      "Testing Set Accuracy with cross-validation (GNB with MaxAbsScaler): 0.9692307692307692\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9615384615384616\n",
      "F1-score: 0.9675\n",
      "ROC AUC: 1.0\n",
      "Recall: 0.975609756097561\n",
      "Kappa Score: 0.935064935064935\n",
      "Log Loss: 0.04805995209637564\n",
      "Matthews Correlation Coefficient: 0.9370425713316364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and GNB classifier for training and testing\n",
    "pipeline_gnb_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_gnb_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (GNB with MaxAbsScaler):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (GNB with MaxAbsScaler):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_gnb_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_gnb = pipeline_gnb_mas.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test_gnb)\n",
    "print(\"Testing Set Accuracy with cross-validation (GNB with MaxAbsScaler):\", test_accuracy_test_gnb)\n",
    "accuracy_mas_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_gnb = pipeline_gnb_mas.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_gnb))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_gnb, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_gnb))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_gnb_mas.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afd6edb8-c4d2-4f2e-9133-08d5f47f5fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN with MaxAbsScaler): [1.         1.         0.95833333 0.82608696 0.91304348 0.86956522\n",
      " 0.7826087  0.91304348 0.82608696 0.91304348]\n",
      "Mean CV accuracy (KNN with MaxAbsScaler): 0.9001811594202896\n",
      "Testing Set Accuracy with cross-validation (KNN with MaxAbsScaler): 0.9661016949152542\n",
      "printing precision\n",
      "0.9545454545454546\n",
      "f1-score\n",
      "0.963032581453634\n",
      "Testing Set Accuracy without cross-validation: 0.9661016949152542\n",
      "Testing Set Accuracy without cross-validation: 0.9661016949152542\n",
      "ROC AUC: 0.9961538461538462\n",
      "recall\n",
      "0.9743589743589743\n",
      "kappa score\n",
      "0.9261576971214017\n",
      "log loss\n",
      "0.2587655960424768\n",
      "MCC\n",
      "0.928693093799487\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_knn_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_knn_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN with MaxAbsScaler):\", cv_scores)\n",
    "print(\"Mean CV accuracy (KNN with MaxAbsScaler):\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_knn_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_knn_mas.predict(X_test)\n",
    "test_accuracy_test_knn_mas = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN with MaxAbsScaler):\", test_accuracy_test_knn_mas)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_knn_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_knn_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_knn_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_knn_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_knn=test_accuracy_test\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_knn_mas.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "809a7ee9-2d37-41df-9a6e-d199539f5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LDA with MaxAbsScaler): [0.6  0.8  0.4  0.5  1.   1.   0.   0.75 1.   1.  ]\n",
      "Mean CV accuracy (LDA with MaxAbsScaler): 0.705\n",
      "Testing Set Accuracy with cross-validation (LDA with MaxAbsScaler): 0.5180722891566265\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.5236737400530505\n",
      "F1-score: 0.5167550782766206\n",
      "ROC AUC: 0.5371989121989122\n",
      "Recall: 0.5231157731157731\n",
      "Kappa Score: 0.04561134534304323\n",
      "Log Loss: 12.116874703575464\n",
      "Matthews Correlation Coefficient: 0.046786186160793386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Import Linear Discriminant Analysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=38)\n",
    "\n",
    "# Define Linear Discriminant Analysis classifier\n",
    "lda = LinearDiscriminantAnalysis()  # Instantiate Linear Discriminant Analysis\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training and testing\n",
    "pipeline_lda_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lda)  # Use LDA classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lda = cross_val_score(pipeline_lda_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LDA with MaxAbsScaler):\", cv_scores_lda)\n",
    "print(\"Mean CV accuracy (LDA with MaxAbsScaler):\", np.mean(cv_scores_lda))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lda_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lda = pipeline_lda_mas.predict(X_test)\n",
    "test_accuracy_test_lda = accuracy_score(y_test, y_pred_test_lda)\n",
    "print(\"Testing Set Accuracy with cross-validation (LDA with MaxAbsScaler):\", test_accuracy_test_lda)\n",
    "accuracy_mas_lda=test_accuracy_test_lda\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lda, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lda = pipeline_lda_mas.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lda))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lda))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lda_mas.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00aca5a9-eb1a-4d5c-bf4b-588f299c3ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LR with MaxAbsScaler): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy (LR with MaxAbsScaler): 1.0\n",
      "Testing Set Accuracy with cross-validation (LR with MaxAbsScaler): 0.9759036144578314\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9755009696186168\n",
      "F1-score: 0.975837753913831\n",
      "ROC AUC: 0.9983812483812484\n",
      "Recall: 0.9763014763014763\n",
      "Kappa Score: 0.9516786338055502\n",
      "Log Loss: 0.18862038148360133\n",
      "Matthews Correlation Coefficient: 0.9518021092898051\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.85, random_state=38)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()  # Instantiate Logistic Regression\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LR classifier for training and testing\n",
    "pipeline_lr_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)  # Use LR classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_lr_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LR with MaxAbsScaler):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (LR with MaxAbsScaler):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_lr_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lr = pipeline_lr_mas.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(\"Testing Set Accuracy with cross-validation (LR with MaxAbsScaler):\", test_accuracy_test_lr)\n",
    "accuracy_mas_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_lr, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_lr = pipeline_lr_mas.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_lr))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lr))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_lr_mas.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0250fd41-4ecd-4a51-aa7a-71e7bfe35d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Random Forest): [1.         0.95454545 1.         0.95454545 0.95454545 0.95454545\n",
      " 0.90909091 1.         1.         1.        ]\n",
      "Mean CV accuracy (Random Forest): 0.9727272727272727\n",
      "Testing Set Accuracy (Random Forest): 0.9452054794520548\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=700, max_depth=15, max_features=None, min_samples_leaf=4, warm_start=True, n_jobs=-1, oob_score=True, random_state=43)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mas', MaxAbsScaler())  # Use MaxAbsScaler instead of Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_rf = cross_val_score(pipeline_rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Random Forest):\", cv_scores_rf)\n",
    "print(\"Mean CV accuracy (Random Forest):\", np.mean(cv_scores_rf))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_rf.predict(X_test)\n",
    "test_accuracy_test_rf = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy (Random Forest):\", test_accuracy_test_rf)\n",
    "accuracy_mas_rf=test_accuracy_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "639766be-fad4-48d6-b9fa-0a1c2846dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (SVM with MaxAbsScaler): [0.75 1.   0.75 1.   1.   1.   1.   1.   1.   0.75]\n",
      "Mean CV accuracy (SVM with MaxAbsScaler): 0.925\n",
      "Testing Set Accuracy with cross-validation (SVM with MaxAbsScaler): 0.9523809523809523\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9518939393939394\n",
      "F1-score: 0.952233524988943\n",
      "ROC AUC: 0.9932329876043511\n",
      "Recall: 0.9526941563369593\n",
      "Kappa Score: 0.9044730856709629\n",
      "Log Loss: 0.1456235166311802\n",
      "Matthews Correlation Coefficient: 0.9045877417867753\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC  # Import Support Vector Classifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Child-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.86, random_state=38)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC(probability=True)  # Instantiate Support Vector Machine with probability=True\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training and testing\n",
    "pipeline_svm_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)  # Use SVM classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_svm_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (SVM with MaxAbsScaler):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (SVM with MaxAbsScaler):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_svm_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_svm = pipeline_svm_mas.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(\"Testing Set Accuracy with cross-validation (SVM with MaxAbsScaler):\", test_accuracy_test_svm)\n",
    "accuracy_mas_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Print precision, F1 score, ROC AUC, recall, kappa score, log loss, and MCC\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test_svm, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test_svm = pipeline_svm_mas.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_test_svm))\n",
    "\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_svm))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_svm_mas.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d23a38b3-b243-42ee-bd1d-d288b7b0d0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "transformers = ['qt', 'pt','nor','mas']\n",
    "models = ['ab', 'rf', 'dt', 'knn', 'gnb', 'lr', 'svm', 'lda']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collecting accuracies for each transformer\n",
    "accuracies = {}\n",
    "for transformer in transformers:\n",
    "    accuracies[transformer] = [globals()[f\"accuracy_{transformer}_{model}\"] for model in models]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for transformer in transformers:\n",
    "    ax.plot(models, accuracies[transformer], label=transformer)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of Models for Different Transformers')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
