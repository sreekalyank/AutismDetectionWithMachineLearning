{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fbdb80-43bb-474e-9fec-bea2c269d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_20036\\2959522157.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data[attr][data[attr] == ''] = np.nan\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_20036\\2959522157.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  categorical_df[col].fillna(mode_val, inplace=True)\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_20036\\2959522157.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing dataset: 0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"D:\\\\Sem 6\\\\Mini Project\\\\autistic+spectrum+disorder+screening+data+for+adolescent\\\\Autism-Adolescent-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert dtype_mapping to list of tuples\n",
    "dtype_tuples = [(col, dtype_mapping[col]) for col in meta.names()]\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming categorical_df contains the one-hot encoded categorical columns\n",
    "# and non_categorical_df contains the bool and float columns\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming joined_df contains the joined DataFrame\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "accuracy_list={}\n",
    "# Split the data into training and testing sets\n",
    "  \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=210)\n",
    "# 7\n",
    "#0.5\n",
    "        \n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "        \n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "        \n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "        \n",
    " # Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "        \n",
    "    \n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', QuantileTransformer(n_quantiles=35,output_distribution='uniform',subsample=60, random_state=91)),\n",
    "    ('oversampler', RandomOverSampler(random_state=12)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='svd',priors=prior_probabilities, store_covariance=True, tol=0.99999999))\n",
    "])\n",
    "        \n",
    "    # Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    # Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=50)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "        \n",
    "    # Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy on testing dataset:\", accuracy)\n",
    "# accuracy_list[i]=accuracy\n",
    "    \n",
    "# Find the key with the maximum value\n",
    "# max_key = max(accuracy_list, key=accuracy_list.get)\n",
    "\n",
    "# Find the maximum value\n",
    "# max_value = accuracy_list[max_key]\n",
    "\n",
    "# Print the result\n",
    "# print(\"Key with maximum value:\", max_key)\n",
    "# print(\"Maximum value:\", max_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d93bc41-f7f2-4438-82cf-077c0ae4b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Feature Importance for Original Features:(Using Info Gain Attribute Evaluator)\n",
      "A1: 0.06592079950177052\n",
      "A2: 0.06592079950177057\n",
      "A3: 0.06592079950177052\n",
      "A4: 0.06592079950177059\n",
      "A5: 0.06592079950177059\n",
      "A6: 0.06592079950177045\n",
      "A7: 0.06592079950177059\n",
      "A8: 0.06592079950177061\n",
      "A9: 0.06592079950177043\n",
      "A10: 0.06592079950177049\n",
      "age: -0.1597517895768759\n",
      "jundice: 0.06592079950177057\n",
      "austim: 0.06592079950177056\n",
      "used: 0.06592079950177043\n",
      "result: 0.6044533969846521\n",
      "gender: -0.1551461047354774\n",
      "ethnicity: -1.0\n",
      "contry: 0.9999999999999999\n",
      "relation: -0.7385077869458502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Define the number of features to select based on Information Gain\n",
    "k = 'all'  # Adjust as needed\n",
    "\n",
    "# Instantiate the SelectKBest transformer with mutual_info_classif as the scoring function\n",
    "ig_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "\n",
    "# Fit the SelectKBest transformer on the training data\n",
    "X_train_selected = ig_selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = ig_selector.get_support(indices=True)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X_train.columns[selected_feature_indices]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Train the LDA model using the selected features\n",
    "lda = LinearDiscriminantAnalysis(solver='svd', priors=prior_probabilities, store_covariance=True, tol=0.99999999)\n",
    "lda.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the feature importance of each selected feature\n",
    "feature_importance = lda.coef_[0]\n",
    "\n",
    "original_feature_importance = {}\n",
    "\n",
    "# Sum up the feature importances for original features\n",
    "for feature_name, importance in zip(feature_names, lda.coef_[0]):\n",
    "    original_feature_name = feature_name.split('_')[0]  # Extract original feature name\n",
    "    if original_feature_name not in original_feature_importance:\n",
    "        original_feature_importance[original_feature_name] = importance\n",
    "    else:\n",
    "        original_feature_importance[original_feature_name] += importance\n",
    "\n",
    "# Scale feature importances to be between 0 and 1 using Min-Max scaling\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_importance = scaler.fit_transform(pd.DataFrame(original_feature_importance.values()))\n",
    "\n",
    "# Print the scaled feature importance of each original feature\n",
    "print(\"Scaled Feature Importance for Original Features:(Using Info Gain Attribute Evaluator)\")\n",
    "for original_feature_name, importance in zip(original_feature_importance.keys(), scaled_importance):\n",
    "    print(f\"{original_feature_name}: {importance[0]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "697727af-51e5-4d38-a9f4-2a81805471fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Feature Importance for Original Features:(Using Info Gain Attribute Evaluator)\n",
      "A1: 0.5329603997508853\n",
      "A2: 0.5329603997508853\n",
      "A3: 0.5329603997508853\n",
      "A4: 0.5329603997508853\n",
      "A5: 0.5329603997508853\n",
      "A6: 0.5329603997508853\n",
      "A7: 0.5329603997508853\n",
      "A8: 0.5329603997508853\n",
      "A9: 0.5329603997508853\n",
      "A10: 0.5329603997508853\n",
      "age: 0.42012410521156207\n",
      "jundice: 0.5329603997508853\n",
      "austim: 0.5329603997508853\n",
      "used: 0.5329603997508853\n",
      "result: 0.8022266984923261\n",
      "gender: 0.4224269476322613\n",
      "ethnicity: 0.0\n",
      "contry: 1.0\n",
      "relation: 0.1307461065270749\n"
     ]
    }
   ],
   "source": [
    "# Define the custom gain ratio attribute evaluator function\n",
    "def gain_ratio_attribute_evaluator(X, y):\n",
    "    lda = LDA()\n",
    "    lda.fit(X, y)\n",
    "    projections = lda.transform(X)\n",
    "    \n",
    "    # Calculate mutual information for each feature\n",
    "    mutual_infos = []\n",
    "    for i in range(projections.shape[1]):\n",
    "        mi = mutual_info_score(projections[:, i], y)\n",
    "        mutual_infos.append(mi)\n",
    "    \n",
    "    # Calculate total mutual information\n",
    "    total_mi = sum(mutual_infos)\n",
    "    \n",
    "    # Calculate split information\n",
    "    split_info = -sum((np.sum(y == c) / len(y)) * np.log2(np.sum(y == c) / len(y)) for c in np.unique(y))\n",
    "    \n",
    "    # Calculate gain ratio for each feature\n",
    "    gain_ratios = []\n",
    "    for mi in mutual_infos:\n",
    "        gain_ratio = mi / split_info if split_info != 0 else 0\n",
    "        gain_ratios.append(gain_ratio)\n",
    "    \n",
    "    # Return gain ratios with None as p-values\n",
    "    return gain_ratios, [None] * len(gain_ratios)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Define the number of features to select based on Information Gain\n",
    "k = 'all'  # Adjust as needed\n",
    "\n",
    "# Instantiate the SelectKBest transformer with mutual_info_classif as the scoring function\n",
    "ig_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "\n",
    "# Fit the SelectKBest transformer on the training data\n",
    "X_train_selected = ig_selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = ig_selector.get_support(indices=True)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X_train.columns[selected_feature_indices]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Train the LDA model using the selected features\n",
    "lda = LinearDiscriminantAnalysis(solver='svd', priors=prior_probabilities, store_covariance=True, tol=0.99999999)\n",
    "lda.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the feature importance of each selected feature\n",
    "feature_importance = lda.coef_[0]\n",
    "\n",
    "original_feature_importance = {}\n",
    "\n",
    "# Sum up the feature importances for original features\n",
    "for feature_name, importance in zip(feature_names, lda.coef_[0]):\n",
    "    original_feature_name = feature_name.split('_')[0]  # Extract original feature name\n",
    "    if original_feature_name not in original_feature_importance:\n",
    "        original_feature_importance[original_feature_name] = importance\n",
    "    else:\n",
    "        original_feature_importance[original_feature_name] += importance\n",
    "\n",
    "# Scale feature importances to be between 0 and 1 using Min-Max scaling\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_importance = scaler.fit_transform(pd.DataFrame(original_feature_importance.values()))\n",
    "\n",
    "# Print the scaled feature importance of each original feature\n",
    "print(\"Scaled Feature Importance for Original Features:(Using Info Gain Attribute Evaluator)\")\n",
    "for original_feature_name, importance in zip(original_feature_importance.keys(), scaled_importance):\n",
    "    print(f\"{original_feature_name}: {importance[0]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "550832d0-ccc5-4fc2-af57-15ed2cb5e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns successfully converted to numeric format.\n",
      "A1_Score                                bool\n",
      "A2_Score                                bool\n",
      "A3_Score                                bool\n",
      "A4_Score                                bool\n",
      "A5_Score                                bool\n",
      "                                      ...   \n",
      "relation_Health care professional    float64\n",
      "relation_Others                      float64\n",
      "relation_Parent                      float64\n",
      "relation_Relative                    float64\n",
      "relation_Self                        float64\n",
      "Length: 62, dtype: object\n"
     ]
    }
   ],
   "source": [
    "non_convertible_columns = []\n",
    "\n",
    "for col in X_train.columns:\n",
    "    try:\n",
    "        X_train[col] = pd.to_numeric(X_train[col])\n",
    "    except ValueError:\n",
    "        non_convertible_columns.append(col)\n",
    "\n",
    "if non_convertible_columns:\n",
    "    print(\"Non-convertible columns found:\", non_convertible_columns)\n",
    "else:\n",
    "    print(\"All columns successfully converted to numeric format.\")\n",
    "\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b957b84d-c99e-4f0b-b2f8-ff4c338e7c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m relieff_selector \u001b[38;5;241m=\u001b[39m ReliefF()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fit ReliefF to the training data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mrelieff_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the indices of selected features\u001b[39;00m\n\u001b[0;32m     10\u001b[0m selected_feature_indices \u001b[38;5;241m=\u001b[39m relieff_selector\u001b[38;5;241m.\u001b[39mtop_features_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\skrebate\\relieff.py:137\u001b[0m, in \u001b[0;36mReliefF.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Number of features in training data\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Number of missing data values in predictor variable matrix.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_data_count \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_X\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Assign internal headers for the features (scikit-learn does not accept external headers from dataset):\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03mThe pre_normalize() function relies on the headers being ordered, e.g., X01, X02, etc.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mIf this is changed, then the sort in the pre_normalize() function needs to be adapted as well. \"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m xlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "from skrebate import ReliefF\n",
    "\n",
    "# Instantiate the ReliefF feature selector\n",
    "relieff_selector = ReliefF()\n",
    "\n",
    "# Fit ReliefF to the training data\n",
    "relieff_selector.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_feature_indices = relieff_selector.top_features_\n",
    "\n",
    "# Select the corresponding columns from X_train and X_test\n",
    "X_train_selected = X_train.iloc[:, selected_feature_indices]\n",
    "X_test_selected = X_test.iloc[:, selected_feature_indices]\n",
    "\n",
    "# Now, retrain your pipeline with the selected features\n",
    "pipeline.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data with selected features\n",
    "y_pred_selected = pipeline.predict(X_test_selected)\n",
    "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
    "print(\"\\nAccuracy on testing dataset with selected features:\", accuracy_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39bfe85f-a23e-4386-a6e3-31e9698d1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_23132\\1301516939.py:43: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data[attr][data[attr] == ''] = np.nan\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation Accuracy (mean): 0.8866666666666667\n",
      "Accuracy on testing dataset(QT_LDA): 0.9615384615384616\n",
      "Other Parameters\n",
      "Precision : 0.9705882352941176\n",
      "Recall : 0.9705882352941176\n",
      "ROC AUC : 0.9575163398692811\n",
      "F1-score : 0.9705882352941176\n",
      "Kappa : 0.9150326797385621\n",
      "Log Loss : 1.3862943611198906\n",
      "MCC : 0.9150326797385621\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 164\u001b[0m\n\u001b[0;32m    161\u001b[0m relief \u001b[38;5;241m=\u001b[39m ReliefF(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Fit ReliefF and transform the data\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m X_relief \u001b[38;5;241m=\u001b[39m \u001b[43mrelief\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Get the indices of the selected features\u001b[39;00m\n\u001b[0;32m    167\u001b[0m selected_features_indices \u001b[38;5;241m=\u001b[39m relief\u001b[38;5;241m.\u001b[39mtop_features_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\skrebate\\relieff.py:239\u001b[0m, in \u001b[0;36mReliefF.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scikit-learn required: Computes the feature importance scores from the training data, then reduces the feature set down to the top `n_features_to_select` features.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m        Reduced feature matrix\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\skrebate\\relieff.py:137\u001b[0m, in \u001b[0;36mReliefF.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Number of features in training data\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Number of missing data values in predictor variable matrix.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_data_count \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_X\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Assign internal headers for the features (scikit-learn does not accept external headers from dataset):\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03mThe pre_normalize() function relies on the headers being ordered, e.g., X01, X02, etc.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mIf this is changed, then the sort in the pre_normalize() function needs to be adapted as well. \"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m xlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from skrebate import ReliefF\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"D:\\\\Sem 6\\\\Mini Project\\\\autistic+spectrum+disorder+screening+data+for+adolescent\\\\Autism-Adolescent-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df.loc[:, col] = categorical_df[col].fillna(mode_val)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df.loc[:, float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=210)\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "\n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "\n",
    "# Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', QuantileTransformer(n_quantiles=35, output_distribution='uniform', subsample=60, random_state=91)),\n",
    "    ('oversampler', RandomOverSampler(random_state=12)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='svd', priors=prior_probabilities, store_covariance=True, tol=0.99999999))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=50)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "print(\"\\nCross-validation Accuracy (mean):\", accuracy_scores.mean())\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on testing dataset(QT_LDA):\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate Cohen's kappa\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, y_pred)\n",
    "\n",
    "# Calculate Matthews correlation coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Other Parameters\")\n",
    "print(\"Precision :\", precision)\n",
    "print(\"Recall :\", recall)\n",
    "print(\"ROC AUC :\", roc_auc)\n",
    "print(\"F1-score :\", f1)\n",
    "print(\"Kappa :\", kappa)\n",
    "print(\"Log Loss :\", logloss)\n",
    "print(\"MCC :\", mcc)\n",
    "\n",
    "# Instantiate ReliefF\n",
    "relief = ReliefF(n_neighbors=10)\n",
    "\n",
    "# Fit ReliefF and transform the data\n",
    "X_relief = relief.fit_transform(X_train.values, y_train.values)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_features_indices = relief.top_features_\n",
    "\n",
    "# Print the selected features\n",
    "selected_features = X_train.columns[selected_features_indices]\n",
    "print(\"Selected Features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d622b13-7924-46b9-8d7e-ee70e06db55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
