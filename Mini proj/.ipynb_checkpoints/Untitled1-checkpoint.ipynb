{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb8e8a1-4861-45c0-b202-15ba433fbea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons\n",
      "0      0   0   0   0   0   0   1   1   0    1        28\n",
      "1      1   1   0   0   0   1   1   0   0    0        36\n",
      "2      1   0   0   0   0   0   1   1   0    1        36\n",
      "3      1   1   1   1   1   1   1   1   1    1        24\n",
      "4      1   1   0   1   1   1   1   1   1    1        20\n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...       ...\n",
      "1049   0   0   0   0   0   0   0   0   0    1        24\n",
      "1050   0   0   1   1   1   0   1   0   1    0        12\n",
      "1051   1   0   1   1   1   1   1   1   1    1        18\n",
      "1052   1   0   0   0   0   0   0   1   0    1        19\n",
      "1053   1   1   0   0   1   1   0   1   1    0        24\n",
      "\n",
      "[1054 rows x 11 columns]\n",
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "print(features)\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353fbc76-94b5-4dd6-9bc4-c61fc240e741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances based on Information Gain:\n",
      "                                            Feature  Importance\n",
      "7                                                A8    0.114544\n",
      "3                                                A4    0.102255\n",
      "4                                                A5    0.099527\n",
      "8                                                A9    0.097973\n",
      "5                                                A6    0.096395\n",
      "9                                               A10    0.094654\n",
      "1                                                A2    0.092205\n",
      "0                                                A1    0.085004\n",
      "6                                                A7    0.074382\n",
      "2                                                A3    0.060700\n",
      "10                                         Age_Mons    0.021336\n",
      "19                                  Ethnicity_asian    0.010722\n",
      "24                                      Jaundice_no    0.009431\n",
      "25                                     Jaundice_yes    0.007396\n",
      "23                            Ethnicity_south asian    0.005353\n",
      "16                                 Ethnicity_Others    0.005188\n",
      "26                           Family_mem_with_ASD_no    0.005118\n",
      "20                                  Ethnicity_black    0.003992\n",
      "21                         Ethnicity_middle eastern    0.003960\n",
      "22                                  Ethnicity_mixed    0.003516\n",
      "11                                            Sex_f    0.003349\n",
      "13                               Ethnicity_Hispanic    0.002999\n",
      "15                          Ethnicity_Native Indian    0.000000\n",
      "28  Who completed the test_Health Care Professional    0.000000\n",
      "31                      Who completed the test_Self    0.000000\n",
      "30                    Who completed the test_Others    0.000000\n",
      "29  Who completed the test_Health care professional    0.000000\n",
      "12                                            Sex_m    0.000000\n",
      "27                          Family_mem_with_ASD_yes    0.000000\n",
      "17                               Ethnicity_Pacifica    0.000000\n",
      "14                                 Ethnicity_Latino    0.000000\n",
      "18                         Ethnicity_White European    0.000000\n",
      "32             Who completed the test_family member    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Train AdaBoost classifier without feature selection for obtaining feature importances\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = np.mean([\n",
    "    tree.feature_importances_ for tree in ada_boost.estimators_\n",
    "], axis=0)\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importance_df = pd.DataFrame({'Feature': result_df.columns,\n",
    "                              'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature Importances based on Information Gain:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71725be6-7dab-4ddf-a79a-2c50f371fc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.98947368 1.         0.98947368 0.96842105 0.98947368\n",
      " 0.98947368 0.98947368 1.         0.9893617 ]\n",
      "Mean CV accuracy: 0.990515117581187\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.9873417721518987\n",
      "f1-score\n",
      "0.9757326007326008\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9655172413793103\n",
      "kappa score\n",
      "0.951487414187643\n",
      "log loss\n",
      "0.02214772948335281\n",
      "MCC\n",
      "0.9526090433773056\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e80d96-3a4f-44b6-bc91-2801f2b06c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n",
      "\n",
      "Feature Importances for Categorical Columns:\n",
      "Sex: 0.003348666145350046\n",
      "Ethnicity: 0.035730864848746643\n",
      "Jaundice: 0.016826863169141844\n",
      "Family_mem_with_ASD: 0.005118422606748412\n",
      "Who completed the test: 0.0\n",
      "\n",
      "Feature Importances for Numerical Columns:\n",
      "A1: 0.08500356826098951\n",
      "A2: 0.09220547606161869\n",
      "A3: 0.060699924065468605\n",
      "A4: 0.10225539522989212\n",
      "A5: 0.09952676701191547\n",
      "A6: 0.0963954593766552\n",
      "A7: 0.07438192171837442\n",
      "A8: 0.11454351632626306\n",
      "A9: 0.09797299533272968\n",
      "A10: 0.09465370098958809\n",
      "Age_Mons: 0.021336458856518196\n"
     ]
    }
   ],
   "source": [
    "print('INFO GAIN ATTRIBUTE EVALUATOR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "# Train AdaBoost classifier without feature selection for obtaining feature importances\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = np.mean([\n",
    "    tree.feature_importances_ for tree in ada_boost.estimators_\n",
    "], axis=0)\n",
    "\n",
    "# Sum feature importances for each categorical column\n",
    "categorical_importances = {}\n",
    "for col in category_features.columns:\n",
    "    cat_mask = result_df.columns.str.startswith(col)\n",
    "    cat_importance = np.sum(feature_importances[cat_mask])\n",
    "    categorical_importances[col] = cat_importance\n",
    "\n",
    "# Print feature importances for each categorical column\n",
    "print(\"\\nFeature Importances for Categorical Columns:\")\n",
    "for col, importance in categorical_importances.items():\n",
    "    print(f\"{col}: {importance}\")\n",
    "\n",
    "# Sum feature importances for numerical columns\n",
    "numerical_importances = {}\n",
    "for col in ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']:\n",
    "    num_importance = np.sum(feature_importances[result_df.columns == col])\n",
    "    numerical_importances[col] = num_importance\n",
    "\n",
    "# Print feature importances for each numerical column\n",
    "print(\"\\nFeature Importances for Numerical Columns:\")\n",
    "for col, importance in numerical_importances.items():\n",
    "    print(f\"{col}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ccbd20-03ea-4da9-9cfa-cb99df550808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__learning_rate': 1.0, 'classifier__n_estimators': 150}\n",
      "Testing Set Accuracy: 0.9751243781094527\n",
      "Precision: 0.9823943661971831\n",
      "F1 Score: 0.970714223271265\n",
      "\n",
      "Feature Importances using Gain Ratio Attribute Evaluator:\n",
      "A1: 0.1721\n",
      "A2: 0.0692\n",
      "A3: 0.0537\n",
      "A4: 0.0559\n",
      "A5: 0.1511\n",
      "A6: 0.0118\n",
      "A7: 0.0000\n",
      "A8: 0.0933\n",
      "A9: 0.3883\n",
      "A10: 0.0000\n",
      "Age_Mons: 0.0000\n",
      "Sex_f_0.0: 0.0000\n",
      "Sex_f_1.0: 0.0046\n",
      "Sex_m_0.0: 0.0000\n",
      "Sex_m_1.0: 0.0000\n",
      "Ethnicity_Hispanic_0.0: 0.0000\n",
      "Ethnicity_Hispanic_1.0: 0.0000\n",
      "Ethnicity_Latino_0.0: 0.0000\n",
      "Ethnicity_Latino_1.0: 0.0000\n",
      "Ethnicity_Native Indian_0.0: 0.0000\n",
      "Ethnicity_Native Indian_1.0: 0.0000\n",
      "Ethnicity_Others_0.0: 0.0000\n",
      "Ethnicity_Others_1.0: 0.0000\n",
      "Ethnicity_Pacifica_0.0: 0.0000\n",
      "Ethnicity_Pacifica_1.0: 0.0000\n",
      "Ethnicity_White European_0.0: 0.0000\n",
      "Ethnicity_White European_1.0: 0.0000\n",
      "Ethnicity_asian_0.0: 0.0000\n",
      "Ethnicity_asian_1.0: 0.0000\n",
      "Ethnicity_black_0.0: 0.0000\n",
      "Ethnicity_black_1.0: 0.0000\n",
      "Ethnicity_middle eastern_0.0: 0.0000\n",
      "Ethnicity_middle eastern_1.0: 0.0000\n",
      "Ethnicity_mixed_0.0: 0.0000\n",
      "Ethnicity_mixed_1.0: 0.0000\n",
      "Ethnicity_south asian_0.0: 0.0000\n",
      "Ethnicity_south asian_1.0: 0.0000\n",
      "Jaundice_no_0.0: 0.0000\n",
      "Jaundice_no_1.0: 0.0000\n",
      "Jaundice_yes_0.0: 0.0000\n",
      "Jaundice_yes_1.0: 0.0000\n",
      "Family_mem_with_ASD_no_0.0: 0.0000\n",
      "Family_mem_with_ASD_no_1.0: 0.0000\n",
      "Family_mem_with_ASD_yes_0.0: 0.0000\n",
      "Family_mem_with_ASD_yes_1.0: 0.0000\n",
      "Who completed the test_Health Care Professional_0.0: 0.0000\n",
      "Who completed the test_Health Care Professional_1.0: 0.0000\n",
      "Who completed the test_Health care professional_0.0: 0.0000\n",
      "Who completed the test_Health care professional_1.0: 0.0000\n",
      "Who completed the test_Others_0.0: 0.0000\n",
      "Who completed the test_Others_1.0: 0.0000\n",
      "Who completed the test_Self_0.0: 0.0000\n",
      "Who completed the test_Self_1.0: 0.0000\n",
      "Who completed the test_family member_0.0: 0.0000\n",
      "Who completed the test_family member_1.0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print('GAIN RATIO ATTRIBUTE EVALUATOR -  not working')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns from original features\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, pd.DataFrame(category_transformed, columns=category_encoded_columns)], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.19, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5, min_samples_split=3, criterion='entropy')\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=300, learning_rate=0.1, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__learning_rate': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = grid_search.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy_test)\n",
    "\n",
    "# Calculate precision and F1 score\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Retrieve the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Calculate and print feature importances using Gain Ratio Attribute Evaluator\n",
    "# Get base estimator (decision tree) from AdaBoost classifier\n",
    "base_estimator = best_model.named_steps['classifier'].estimators_[0]\n",
    "\n",
    "# Calculate feature importances\n",
    "if hasattr(base_estimator, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    feature_importances = base_estimator.feature_importances_\n",
    "\n",
    "    # Get feature names from preprocessor\n",
    "    num_feature_names = list(best_model.named_steps['preprocessor'].transformers_[0][2])\n",
    "    cat_feature_names = list(best_model.named_steps['preprocessor'].transformers_[1][1]['onehot'].get_feature_names_out(category_encoded_columns))\n",
    "\n",
    "    # Combine numerical and categorical feature names\n",
    "    feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "    # Print feature importances\n",
    "    print(\"\\nFeature Importances using Gain Ratio Attribute Evaluator:\")\n",
    "    for name, importance in zip(feature_names, feature_importances):\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"Base estimator does not support feature importances.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
