{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a91475b5-a921-4309-a028-3931bf62db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons\n",
      "0      0   0   0   0   0   0   1   1   0    1        28\n",
      "1      1   1   0   0   0   1   1   0   0    0        36\n",
      "2      1   0   0   0   0   0   1   1   0    1        36\n",
      "3      1   1   1   1   1   1   1   1   1    1        24\n",
      "4      1   1   0   1   1   1   1   1   1    1        20\n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...       ...\n",
      "1049   0   0   0   0   0   0   0   0   0    1        24\n",
      "1050   0   0   1   1   1   0   1   0   1    0        12\n",
      "1051   1   0   1   1   1   1   1   1   1    1        18\n",
      "1052   1   0   0   0   0   0   0   1   0    1        19\n",
      "1053   1   1   0   0   1   1   0   1   1    0        24\n",
      "\n",
      "[1054 rows x 11 columns]\n",
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "print(features)\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_ab=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d72f436-3d2f-4251-9e6a-f382bf742213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 0.96470588 0.94117647 0.94047619 0.9047619  0.95238095\n",
      " 0.94047619 0.91666667 0.91666667 0.96428571]\n",
      "Mean CV accuracy: 0.939453781512605\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "ROC AUC: 0.9994896917738314\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.12395449227564767\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_qt_rf=test_accuracy_test\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e0b604-c263-48de-bec5-e32d24ef83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.87058824 0.92941176 0.95294118 0.92857143 0.95238095 0.91666667\n",
      " 0.89285714 0.9047619  0.9047619  0.86904762]\n",
      "Mean CV accuracy: 0.9121988795518208\n",
      "Testing Set Accuracy  : 0.966824644549763\n",
      "Testing Set Accuracy  : 0.966824644549763\n",
      "printing precision\n",
      "0.9580835331734612\n",
      "f1-score\n",
      "0.9627217889503041\n",
      "ROC AUC: 0.9679016125739947\n",
      "recall\n",
      "0.9679016125739948\n",
      "kappa score\n",
      "0.9254605097148625\n",
      "log loss\n",
      "1.1957610129090999\n",
      "MCC\n",
      "0.9259330944845972\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=None, min_samples_split=2,random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=100, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_dt=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e9ba4fc-a0c1-44f0-a506-3979a82f34aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 0.90588235 0.94117647 0.91666667 0.94047619 0.92857143\n",
      " 0.92857143 0.92857143 0.95238095 0.92857143]\n",
      "Mean CV accuracy: 0.9323809523809524\n",
      "Testing Set Accuracy  : 0.9715639810426541\n",
      "Testing Set Accuracy  : 0.9715639810426541\n",
      "printing precision\n",
      "0.96\n",
      "f1-score\n",
      "0.9683752997601918\n",
      "ROC AUC: 0.997040212288222\n",
      "recall\n",
      "0.9788732394366197\n",
      "kappa score\n",
      "0.9368074273734651\n",
      "log loss\n",
      "0.13239564713205948\n",
      "MCC\n",
      "0.9386835252434019\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=11, weights='distance',algorithm='auto', p=2, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=100,subsample=500,random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_knn=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70700ded-f4e2-45c7-96e5-a12bb6e97cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 2.154434690031882}\n",
      "Best Score: 0.9535834266517359\n",
      "Testing Set Accuracy: 0.9811320754716981\n",
      "Precision: 0.967741935483871\n",
      "F1-score: 0.9767543859649122\n",
      "ROC AUC: 0.9977608598298253\n",
      "Recall: 0.987012987012987\n",
      "Kappa Score: 0.9535291538798772\n",
      "Log Loss: 0.16175233379085582\n",
      "Matthews Correlation Coefficient: 0.9545604164247246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=300, random_state=69))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(5,-9, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_qt_gnb=test_accuracy\n",
    "\n",
    "# Additional imports\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "#QT_GNB_Toddlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e8bbd33-51b7-4597-a542-f287cc5751fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98611111 0.98611111 0.98611111 1.         0.98611111 0.98611111\n",
      " 1.         0.98591549 1.         1.        ]\n",
      "Mean CV accuracy: 0.9916471048513301\n",
      "Testing Set Accuracy  : 0.9881656804733728\n",
      "printing precision\n",
      "0.9861843449826282\n",
      "f1-score\n",
      "0.9861843449826282\n",
      "ROC AUC: 0.9997138769670959\n",
      "recall\n",
      "0.9861843449826282\n",
      "kappa score\n",
      "0.9723686899652565\n",
      "log loss\n",
      "0.060630595062107835\n",
      "MCC\n",
      "0.9723686899652565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.32, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_lr=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#QT_LR_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c69c4fbc-946c-420e-9487-617f1e72d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97849462 1.         0.97849462 0.98924731 0.96774194 0.96774194\n",
      " 0.98924731 1.         0.93478261 0.97826087]\n",
      "Mean CV accuracy: 0.9784011220196354\n",
      "Testing Set Accuracy  : 0.984251968503937\n",
      "printing precision\n",
      "0.9887640449438202\n",
      "f1-score\n",
      "0.981497668997669\n",
      "ROC AUC: 0.9997126436781609\n",
      "recall\n",
      "0.975\n",
      "kappa score\n",
      "0.9630061170987474\n",
      "log loss\n",
      "0.027607827447274497\n",
      "MCC\n",
      "0.9636657539796973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.12, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_svm=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#QT_SVM_Toddlers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a4a563e-afb1-43eb-a428-5a6e5b425a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92631579 0.96842105 0.92631579 0.94736842 0.96842105 0.94736842\n",
      " 0.91578947 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9472788353863383\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "ROC AUC: 0.9991043439319302\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "log loss\n",
      "0.06658456169079688\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_lda=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#QT_LDA_Toddlers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4aacd0b-2b3f-4c9d-9a9c-1dd6655ee021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_ab=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d06cefa-6edd-45a4-a4cf-040be574d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 0.96470588 0.94117647 0.94047619 0.9047619  0.95238095\n",
      " 0.94047619 0.91666667 0.91666667 0.96428571]\n",
      "Mean CV accuracy: 0.939453781512605\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "ROC AUC: 0.9994896917738313\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.12435094226362926\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_rf=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8531ce7-2bcc-4650-9577-214075eec5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.89873418 0.89873418 0.89873418 0.91139241 0.94936709 0.93670886\n",
      " 0.89873418 0.84810127 0.89873418 0.91139241]\n",
      "Mean CV accuracy: 0.9050632911392403\n",
      "Testing Set Accuracy  : 0.9507575757575758\n",
      "Testing Set Accuracy  : 0.9507575757575758\n",
      "printing precision\n",
      "0.9369764270407169\n",
      "f1-score\n",
      "0.9434326119562888\n",
      "Testing Set Accuracy without cross-validation: 0.9507575757575758\n",
      "Testing Set Accuracy without cross-validation: 0.9507575757575758\n",
      "ROC AUC: 0.9508844813722862\n",
      "recall\n",
      "0.9508844813722862\n",
      "kappa score\n",
      "0.8869118228548833\n",
      "log loss\n",
      "1.7748768714337992\n",
      "MCC\n",
      "0.8877519691404088\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=None, min_samples_split=2,random_state=35)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_dt=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12351439-5d25-47ee-9612-0f3ad2cb0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96385542 0.97590361 0.91463415 0.96341463 0.96341463 0.96341463\n",
      " 0.91463415 0.97560976 0.92682927 0.92682927]\n",
      "Mean CV accuracy: 0.9488539523949457\n",
      "Testing Set Accuracy  : 0.9827586206896551\n",
      "Testing Set Accuracy  : 0.9827586206896551\n",
      "printing precision\n",
      "0.974025974025974\n",
      "f1-score\n",
      "0.9802972399150742\n",
      "Testing Set Accuracy  : 0.9827586206896551\n",
      "Testing Set Accuracy without cross-validation: 0.9827586206896551\n",
      "ROC AUC: 0.9997415352804343\n",
      "recall\n",
      "0.9874213836477987\n",
      "kappa score\n",
      "0.9606078614483403\n",
      "log loss\n",
      "0.09197860141703099\n",
      "MCC\n",
      "0.96135403706384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=11, weights='distance', algorithm='auto', p=2, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_knn=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c275eab-0807-4d17-aaec-85364c09f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 2.154434690031884}\n",
      "Best Score: 0.9655811403508773\n",
      "Testing Set Accuracy: 0.9894736842105263\n",
      "Precision: 0.9827586206896552\n",
      "F1-score: 0.9874686716791979\n",
      "ROC AUC: 1.0\n",
      "Recall: 0.9925373134328358\n",
      "Kappa Score: 0.9749406489053021\n",
      "Log Loss: 0.15181935596283097\n",
      "Matthews Correlation Coefficient: 0.975246910420175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(1,-1, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_pt_gnb=test_accuracy_cv\n",
    "# Evaluate other metrics\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "\n",
    "#PT_GNB_Toddlers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09d75c57-04a7-4b3d-994e-7cf09a012ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py:839: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 136, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_response.py\", line 194, in _get_response_values\n",
      "    y_pred = prediction_method(X)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\pipeline.py\", line 514, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 827, in transform\n",
      "    Xs = self._fit_transform(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 681, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\pipeline.py\", line 940, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\pipeline.py\", line 696, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1027, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 200, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories [1.0] in column 4 during transform\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [ 1. nan  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Mean CV accuracy: nan\n",
      "Testing Set Accuracy  : 0.9981024667931688\n",
      "printing precision\n",
      "0.9986072423398329\n",
      "f1-score\n",
      "0.9978189704050424\n",
      "ROC AUC: 0.999983471620773\n",
      "recall\n",
      "0.9970414201183432\n",
      "kappa score\n",
      "0.995637958862724\n",
      "log loss\n",
      "0.037436289606112505\n",
      "MCC\n",
      "0.9956474312001843\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transformer', PowerTransformer())  # Change Normalizer to PowerTransformer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=69))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_lr=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#PT_LR_Toddlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a5b6a49-e7f9-4452-8e45-a06ec57511c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.98947368 1.         0.98947368 0.96842105 0.98947368\n",
      " 0.98947368 0.98947368 1.         0.9893617 ]\n",
      "Mean CV accuracy: 0.990515117581187\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.9873417721518987\n",
      "f1-score\n",
      "0.9757326007326008\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9655172413793103\n",
      "kappa score\n",
      "0.951487414187643\n",
      "log loss\n",
      "0.021960919354479268\n",
      "MCC\n",
      "0.9526090433773056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))  # Use PowerTransformer instead of QuantileTransformer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True))  # Support Vector Classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_svm=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True))  # Support Vector Classifier\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6b5cef7-4ab0-4e6d-b965-d2a42b84d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92631579 0.96842105 0.92631579 0.94736842 0.96842105 0.94736842\n",
      " 0.91578947 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9472788353863383\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "ROC AUC: 0.9995521719659651\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "log loss\n",
      "0.06566255495861267\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))  # Use PowerTransformer instead of QuantileTransformer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_lda=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#PT_LDA_Toddler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c0919ec-63cc-4768-9c2d-f788048bf27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__learning_rate': 1.0, 'classifier__n_estimators': 150}\n",
      "Testing Set Accuracy: 0.9900497512437811\n",
      "printing precision\n",
      "0.9885378649635037\n",
      "f1-score\n",
      "0.9885378649635037\n",
      "Testing Set Accuracy  : 0.9751243781094527\n",
      "Testing Set Accuracy without cross-validation: 0.9751243781094527\n",
      "ROC AUC: 0.9692632299270073\n",
      "recall\n",
      "0.9692632299270073\n",
      "kappa score\n",
      "0.9424497509019069\n",
      "log loss\n",
      "0.896608293261621\n",
      "MCC\n",
      "0.9425121855645885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.19, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),\n",
    "                               n_estimators=100, learning_rate=0.5, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__learning_rate': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = grid_search.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7bfc6fc2-434d-45c5-ad1f-175ce258d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Random Forest): {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "Testing Set Accuracy (Random Forest): 0.9835390946502057\n",
      "printing precision\n",
      "0.9842836257309941\n",
      "f1-score\n",
      "0.9804190169218372\n",
      "Testing Set Accuracy  : 0.9835390946502057\n",
      "Testing Set Accuracy without cross-validation: 0.9835390946502057\n",
      "ROC AUC: 0.9708539900847594\n",
      "recall\n",
      "0.9767711498480729\n",
      "kappa score\n",
      "0.9608411892675852\n",
      "log loss\n",
      "0.8899667503485718\n",
      "MCC\n",
      "0.9610254129675445\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.23, random_state=42)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters (Random Forest):\", grid_search_rf.best_params_)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_rf = grid_search_rf.predict(X_test)\n",
    "test_accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(\"Testing Set Accuracy (Random Forest):\", test_accuracy_test_rf)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test_rf, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test_rf,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_nor_rf=test_accuracy_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test_rf,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test_rf))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "754cd947-c16d-49ed-9b0a-440dc8079d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92771084 0.95180723 0.85365854 0.95121951 0.90243902 0.95121951\n",
      " 0.86585366 0.92682927 0.95121951 0.93902439]\n",
      "Mean CV accuracy: 0.9220981486923303\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "printing precision\n",
      "0.9700180925303696\n",
      "f1-score\n",
      "0.9700180925303696\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "ROC AUC: 0.9700180925303697\n",
      "recall\n",
      "0.9700180925303696\n",
      "kappa score\n",
      "0.9400361850607392\n",
      "log loss\n",
      "0.9321634497185471\n",
      "MCC\n",
      "0.9400361850607392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='log_loss',splitter='random',max_depth=None, min_samples_split=2,random_state=35)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_dt=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "861cd12d-3dfd-4194-b41e-2527e4c3efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92708333 0.89583333 0.90625    0.91666667 0.86458333 0.88541667\n",
      " 0.875      0.9375     0.89583333 0.85263158]\n",
      "Mean CV accuracy: 0.8956798245614035\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "printing precision\n",
      "0.9827586206896552\n",
      "f1-score\n",
      "0.9874686716791979\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "ROC AUC: 0.9973347547974414\n",
      "recall\n",
      "0.9925373134328358\n",
      "kappa score\n",
      "0.9749406489053021\n",
      "log loss\n",
      "0.13411050267706187\n",
      "MCC\n",
      "0.975246910420175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=11, weights='distance', algorithm='auto', p=2, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_nor_knn=test_accuracy_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5761fb13-fb4b-4c67-af98-5b84c6afe6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 0.21544346900318845}\n",
      "Best Score: 0.9209182530795073\n",
      "Testing Set Accuracy: 0.9716981132075472\n",
      "Precision: 0.953125\n",
      "F1-score: 0.9654760612311366\n",
      "ROC AUC: 0.9914912673533363\n",
      "Recall: 0.9805194805194806\n",
      "Kappa Score: 0.9310195227765726\n",
      "Log Loss: 0.1067825093691721\n",
      "Matthews Correlation Coefficient: 0.9332424971257783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(1,-2, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_nor_gnb=test_accuracy_cv\n",
    "# Evaluate other metrics\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "#Norma_GNB_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0a358d0-bea8-4a45-b098-e7508de6d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.     0.975  0.9875 0.975  1.     1.     1.     0.9875 0.9875 0.9875]\n",
      "Mean CV accuracy: 0.99\n",
      "Testing Set Accuracy  : 0.9881422924901185\n",
      "printing precision\n",
      "0.9817073170731707\n",
      "f1-score\n",
      "0.986335403726708\n",
      "ROC AUC: 0.999418012512731\n",
      "recall\n",
      "0.9913793103448276\n",
      "kappa score\n",
      "0.9726752349065774\n",
      "log loss\n",
      "0.08585858958863242\n",
      "MCC\n",
      "0.9730385588484597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.24, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Change QuantileTransformer to Normalizer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_lr=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#Norma_LR_Toddler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9993e06-0a18-4e29-abab-961c37c96381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97916667 0.96875    1.         0.92708333 0.96875    0.98958333\n",
      " 0.96875    0.94791667 0.96875    0.94736842]\n",
      "Mean CV accuracy: 0.9666118421052632\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "printing precision\n",
      "0.9855072463768115\n",
      "f1-score\n",
      "0.974128540305011\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9642857142857143\n",
      "kappa score\n",
      "0.9482852476864453\n",
      "log loss\n",
      "0.06471067908005945\n",
      "MCC\n",
      "0.949555851279846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Preprocessing pipeliney\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_svm=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.decision_function(X_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))  \n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "##Norma_SVM_Toddler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "512b3232-136f-40b0-8c13-2dc6d55f03cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.91578947 0.95789474 0.91578947 0.94736842 0.96842105 0.93684211\n",
      " 0.90526316 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9420156774916013\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LDA())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_lda=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LDA())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "##Norma_LDA_Toddler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dce8c45-4fed-4f47-825b-6f90be974df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mas', MaxAbsScaler())  # Replace 'quantile' with 'mas'\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_mas_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01780e21-1bdb-423b-973d-2b7355d60501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 50}\n",
      "Testing Set Accuracy with best hyperparameters: 0.9827586206896551\n",
      "printing precision\n",
      "0.980012061686913\n",
      "f1-score\n",
      "0.980012061686913\n",
      "Testing Set Accuracy  : 0.9913793103448276\n",
      "Testing Set Accuracy without cross-validation: 0.9913793103448276\n",
      "ROC AUC: 0.9900060308434564\n",
      "recall\n",
      "0.9900060308434565\n",
      "kappa score\n",
      "0.980012061686913\n",
      "log loss\n",
      "0.31072114990618255\n",
      "MCC\n",
      "0.980012061686913\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with MaxAbsScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=35))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [10, 15, 20],\n",
    "    \n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from the grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the best model on the separate testing set\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "test_accuracy_test_best = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with best hyperparameters:\", test_accuracy_test_best)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_rf=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aaf38171-e820-4d5d-9214-520d9f3ebc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96875    0.91666667 0.88541667 0.96875    0.91666667 0.9375\n",
      " 0.91666667 0.91666667 0.85416667 0.91578947]\n",
      "Mean CV accuracy: 0.919703947368421\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "printing precision\n",
      "0.9746801705756929\n",
      "f1-score\n",
      "0.9746801705756929\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "ROC AUC: 0.974680170575693\n",
      "recall\n",
      "0.9746801705756929\n",
      "kappa score\n",
      "0.9493603411513859\n",
      "log loss\n",
      "0.7588137555603612\n",
      "MCC\n",
      "0.9493603411513859\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=None, min_samples_split=2, random_state=35)\n",
    "\n",
    "# Preprocessing pipeline with MaxAbsScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_mas_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c60d395-38a2-4d9c-aef5-02d4bdcc06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross-validation scores: [0.9375     0.94791667 0.88541667 0.96875    0.86458333 0.91666667\n",
      " 0.91666667 0.92708333 0.875      0.90526316]\n",
      "Mean KNN CV accuracy: 0.9144846491228069\n",
      "Testing Set Accuracy with KNN (cross-validation): 0.9789473684210527\n",
      "printing precision\n",
      "0.9666666666666667\n",
      "f1-score\n",
      "0.9751828631138977\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "Testing Set Accuracy without cross-validation: 0.9789473684210527\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9821428571428572\n",
      "kappa score\n",
      "0.9744142203070294\n",
      "log loss\n",
      "0.03724907074530016\n",
      "MCC\n",
      "0.9747333184090953\n"
     ]
    }
   ],
   "source": [
    "#### import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline with MaxAbsScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set using KNN\n",
    "cv_scores_knn = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"KNN Cross-validation scores:\", cv_scores_knn)\n",
    "print(\"Mean KNN CV accuracy:\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using KNN\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy with KNN (cross-validation):\", test_accuracy_cv)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "precision=precision_score(y_test, y_pred_cv, average='macro')\n",
    "print(precision)\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_cv,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_mas_knn=test_accuracy_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf885ecd-98dd-487c-87fc-bcca50adce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 2.154434690031884}\n",
      "Best Score: 0.9578163493840985\n",
      "Testing Set Accuracy: 0.9716981132075472\n",
      "Precision: 0.9600877192982455\n",
      "F1-score: 0.9647723496178133\n",
      "ROC AUC: 0.9986565158978953\n",
      "Recall: 0.9697716077026421\n",
      "Kappa Score: 0.9295525033229951\n",
      "Log Loss: 0.1818012071681241\n",
      "Matthews Correlation Coefficient: 0.9298088998906792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('max_abs_scaler', MaxAbsScaler())  # Replace Normalizer with MaxAbsScaler\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(1,-2, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_mas_gnb=test_accuracy\n",
    "# Evaluate other metrics\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "#Norma_GNB_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05b626a4-56b4-4d9b-bb26-e8d2cedc043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98611111 0.98611111 0.98611111 1.         0.98611111 0.98611111\n",
      " 1.         0.98591549 1.         1.        ]\n",
      "Mean CV accuracy: 0.9916471048513301\n",
      "Testing Set Accuracy  : 0.9881656804733728\n",
      "printing precision\n",
      "0.9861843449826282\n",
      "f1-score\n",
      "0.9861843449826282\n",
      "ROC AUC: 0.9996730022481095\n",
      "recall\n",
      "0.9861843449826282\n",
      "kappa score\n",
      "0.9723686899652565\n",
      "log loss\n",
      "0.060362802581865146\n",
      "MCC\n",
      "0.9723686899652565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.32, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_mas_lr=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#MSA_LR_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84619f50-dc73-4d40-9108-8149b2a6f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97849462 1.         0.97849462 0.98924731 0.96774194 0.96774194\n",
      " 1.         1.         0.94565217 0.97826087]\n",
      "Mean CV accuracy: 0.9805633473585788\n",
      "Testing Set Accuracy  : 0.984251968503937\n",
      "printing precision\n",
      "0.9887640449438202\n",
      "f1-score\n",
      "0.981497668997669\n",
      "ROC AUC: 0.9994252873563217\n",
      "recall\n",
      "0.975\n",
      "kappa score\n",
      "0.9630061170987474\n",
      "log loss\n",
      "0.026425866315960663\n",
      "MCC\n",
      "0.9636657539796973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.12, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_mas_svm=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#MSA_SVM_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8bcc5f4d-67c6-49ec-ba0f-34df4e247399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92631579 0.96842105 0.92631579 0.94736842 0.97894737 0.94736842\n",
      " 0.92631579 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9493840985442329\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "ROC AUC: 0.9995521719659651\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "log loss\n",
      "0.06453460393260697\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_mas_lda=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#MSA_LDA_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e61aba2d-afc6-402d-a534-2c5c548c160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "transformers = ['qt', 'pt','nor','mas']\n",
    "models = ['ab', 'rf', 'dt', 'knn', 'gnb', 'lr', 'svm', 'lda']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collecting accuracies for each transformer\n",
    "accuracies = {}\n",
    "for transformer in transformers:\n",
    "    accuracies[transformer] = [globals()[f\"accuracy_{transformer}_{model}\"] for model in models]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for transformer in transformers:\n",
    "    ax.plot(models, accuracies[transformer], label=transformer)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of Models for Different Transformers')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
