{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc5123b-0838-4776-bb5c-d1fb0d5c575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_13888\\805597401.py:37: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data[attr][data[attr] == ''] = np.nan\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_13888\\805597401.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  categorical_df[col].fillna(mode_val, inplace=True)\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_13888\\805597401.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing dataset: 0.9615384615384616\n",
      "Scaled Feature Importance for Original Features:(Using Info Gain Attribute Evaluator)\n",
      "A1: 0.06592079950177052\n",
      "A2: 0.06592079950177057\n",
      "A3: 0.06592079950177052\n",
      "A4: 0.06592079950177059\n",
      "A5: 0.06592079950177059\n",
      "A6: 0.06592079950177045\n",
      "A7: 0.06592079950177059\n",
      "A8: 0.06592079950177061\n",
      "A9: 0.06592079950177043\n",
      "A10: 0.06592079950177049\n",
      "age: -0.1597517895768759\n",
      "jundice: 0.06592079950177057\n",
      "austim: 0.06592079950177056\n",
      "used: 0.06592079950177043\n",
      "result: 0.6044533969846521\n",
      "gender: -0.1551461047354774\n",
      "ethnicity: -1.0\n",
      "contry: 0.9999999999999999\n",
      "relation: -0.7385077869458502\n"
     ]
    }
   ],
   "source": [
    "print('INFO GAIN ATTRIBUTE EVALUATOR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"D:\\\\Sem 6\\\\Mini Project\\\\autistic+spectrum+disorder+screening+data+for+adolescent\\\\Autism-Adolescent-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert dtype_mapping to list of tuples\n",
    "dtype_tuples = [(col, dtype_mapping[col]) for col in meta.names()]\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming categorical_df contains the one-hot encoded categorical columns\n",
    "# and non_categorical_df contains the bool and float columns\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming joined_df contains the joined DataFrame\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "accuracy_list={}\n",
    "# Split the data into training and testing sets\n",
    "  \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=210)\n",
    "# 7\n",
    "#0.5\n",
    "        \n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "        \n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "        \n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "        \n",
    " # Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "        \n",
    "    \n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', QuantileTransformer(n_quantiles=35,output_distribution='uniform',subsample=60, random_state=91)),\n",
    "    ('oversampler', RandomOverSampler(random_state=12)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='svd',priors=prior_probabilities, store_covariance=True, tol=0.99999999))\n",
    "])\n",
    "        \n",
    "    # Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    # Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=50)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "        \n",
    "    # Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy on testing dataset:\", accuracy)\n",
    "# accuracy_list[i]=accuracy\n",
    "    \n",
    "# Find the key with the maximum value\n",
    "# max_key = max(accuracy_list, key=accuracy_list.get)\n",
    "\n",
    "# Find the maximum value\n",
    "# max_value = accuracy_list[max_key]\n",
    "\n",
    "# Print the result\n",
    "# print(\"Key with maximum value:\", max_key)\n",
    "# print(\"Maximum value:\", max_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Define the number of features to select based on Information Gain\n",
    "k = 'all'  # Adjust as needed\n",
    "\n",
    "# Instantiate the SelectKBest transformer with mutual_info_classif as the scoring function\n",
    "ig_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "\n",
    "# Fit the SelectKBest transformer on the training data\n",
    "X_train_selected = ig_selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = ig_selector.get_support(indices=True)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X_train.columns[selected_feature_indices]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Train the LDA model using the selected features\n",
    "lda = LinearDiscriminantAnalysis(solver='svd', priors=prior_probabilities, store_covariance=True, tol=0.99999999)\n",
    "lda.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the feature importance of each selected feature\n",
    "feature_importance = lda.coef_[0]\n",
    "\n",
    "original_feature_importance = {}\n",
    "\n",
    "# Sum up the feature importances for original features\n",
    "for feature_name, importance in zip(feature_names, lda.coef_[0]):\n",
    "    original_feature_name = feature_name.split('_')[0]  # Extract original feature name\n",
    "    if original_feature_name not in original_feature_importance:\n",
    "        original_feature_importance[original_feature_name] = importance\n",
    "    else:\n",
    "        original_feature_importance[original_feature_name] += importance\n",
    "\n",
    "# Scale feature importances to be between 0 and 1 using Min-Max scaling\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_importance = scaler.fit_transform(pd.DataFrame(original_feature_importance.values()))\n",
    "\n",
    "# Print the scaled feature importance of each original feature\n",
    "print(\"Scaled Feature Importance for Original Features:(Using Info Gain Attribute Evaluator)\")\n",
    "for original_feature_name, importance in zip(original_feature_importance.keys(), scaled_importance):\n",
    "    print(f\"{original_feature_name}: {importance[0]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ff6ae-f140-4ca5-b01f-becedc90d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RELIEF F ATTRIBUTE EVALUATOR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skrebate import ReliefF\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"D:\\\\SEM 6\\\\Mini Project\\\\Autism\\\\Datasets\\\\autistic+spectrum+disorder+screening+data+for+adolescent\\\\Autism-Adolescent-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming joined_df contains the joined DataFrame\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "accuracy_list={}\n",
    "# Split the data into training and testing sets\n",
    "  \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=210)\n",
    "        \n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "        \n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "        \n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "        \n",
    " # Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "        \n",
    "    # Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', QuantileTransformer(n_quantiles=35,output_distribution='uniform',subsample=60, random_state=91)),\n",
    "    ('oversampler', RandomOverSampler(random_state=12)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='svd',priors=prior_probabilities, store_covariance=True, tol=0.99999999))\n",
    "])\n",
    "        \n",
    "    # Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    # Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=50)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "print(\"\\nCross-validation Accuracy (mean):\", accuracy_scores.mean())\n",
    "        \n",
    "    # Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on testing dataset(QT_LDA):\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate Cohen's kappa\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, y_pred)\n",
    "\n",
    "# Calculate Matthews correlation coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Other Parameters\")\n",
    "print(\"Precision :\", precision)\n",
    "print(\"Recall :\", recall)\n",
    "print(\"ROC AUC :\", roc_auc)\n",
    "print(\"F1-score :\", f1)\n",
    "print(\"Kappa :\", kappa)\n",
    "print(\"Log Loss :\", logloss)\n",
    "print(\"MCC :\", mcc)\n",
    "\n",
    "# Print feature importance of each feature by name\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_names = X_train.columns\n",
    "for feature_name, importance in zip(feature_names, pipeline.named_steps['classifier'].coef_[0]):\n",
    "    print(f\"{feature_name}: Importance = {importance}\")\n",
    "\n",
    "# Print feature importance of each feature by name\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_names = X_train.columns\n",
    "for feature_name, importance in zip(feature_names, pipeline.named_steps['classifier'].coef_[0]):\n",
    "    scaled_importance = importance * 1e16  # Scale up the importance value\n",
    "    importance_formatted = \"{:.5f}\".format(scaled_importance)  # Format scaled importance value to 5 decimal points\n",
    "    print(f\"{feature_name}: Importance = {importance_formatted}\")\n",
    "\n",
    "# Get the maximum absolute value of feature importance\n",
    "max_abs_importance = np.max(np.abs(pipeline.named_steps['classifier'].coef_))\n",
    "\n",
    "# Print feature importance of each feature by name normalized between -1 and 1\n",
    "print(\"\\nFeature Importance (Normalized between -1 and 1):\")\n",
    "for feature_name, importance in zip(feature_names, pipeline.named_steps['classifier'].coef_[0]):\n",
    "    normalized_importance = importance / max_abs_importance  # Normalize importance value between -1 and 1\n",
    "    print(f\"{feature_name}: Importance = {normalized_importance:.5f}\")\n",
    "# Group the feature importances by original feature names\n",
    "original_feature_importances = {}\n",
    "for feature_name, importance in zip(feature_names, pipeline.named_steps['classifier'].coef_[0]):\n",
    "    original_feature_name = feature_name.split('_')[0]  # Extract the original feature name\n",
    "    original_feature_importances.setdefault(original_feature_name, []).append(importance)\n",
    "\n",
    "# Sum up the feature importances for each original feature\n",
    "summed_feature_importances = {original_feature_name: sum(importances) for original_feature_name, importances in original_feature_importances.items()}\n",
    "\n",
    "# Print the summed feature importances\n",
    "print(\"\\nSummed Feature Importances for Original Features:\")\n",
    "for original_feature_name, importance_sum in summed_feature_importances.items():\n",
    "    print(f\"{original_feature_name}: Importance = {importance_sum:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d0b21-0ef2-4f29-80c7-cf99551079d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAE NOT DONE ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fd2bf-2953-49f8-ad4b-12dfca9c5d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
