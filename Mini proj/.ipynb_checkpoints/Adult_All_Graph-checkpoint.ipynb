{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5042daf6-e41a-41c5-ae51-7a0fb069ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         1.         1.         1.         1.         1.\n",
      " 1.         0.98214286 1.         0.98214286]\n",
      "Mean CV accuracy: 0.9964285714285716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation: 0.9858156028368794\n",
      "printing precision\n",
      "0.9736842105263157\n",
      "f1-score\n",
      "0.9816787941787941\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9904761904761905\n",
      "kappa score\n",
      "0.9633671083398285\n",
      "log loss\n",
      "0.034967170588906714\n",
      "MCC\n",
      "0.9640141639367529\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=32, learning_rate=0.1, algorithm='SAMME.R', random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=10)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_qt_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "763aa1ee-d3b2-4f61-9ddc-ec1671e3538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree): [0.98387097 1.         0.98387097 1.         0.98387097 0.91935484\n",
      " 1.         1.         0.98360656 1.        ]\n",
      "Mean CV accuracy (Decision Tree): 0.9854574299312533\n",
      "Testing Set Accuracy with cross-validation (Decision Tree): 0.9886363636363636\n",
      "printing precision\n",
      "0.9814814814814814\n",
      "f1-score\n",
      "0.9865009970854426\n",
      "Testing Set Accuracy without cross-validation: 0.9886363636363636\n",
      "Testing Set Accuracy without cross-validation: 0.9886363636363636\n",
      "ROC AUC: 0.9919354838709677\n",
      "recall\n",
      "0.9919354838709677\n",
      "kappa score\n",
      "0.9730061349693252\n",
      "log loss\n",
      "0.40958697033087693\n",
      "MCC\n",
      "0.9733608284033275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.125, random_state=33)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=None, min_samples_split=4,random_state=39)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_dt = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree):\", cv_scores_dt)\n",
    "print(\"Mean CV accuracy (Decision Tree):\", np.mean(cv_scores_dt))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_dt\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_qt_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed26eb5f-ea12-44da-931f-0e1aba38f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Gaussian Naive Bayes): [0.98387097 0.96774194 0.96774194 1.         1.         0.98387097\n",
      " 0.96721311 1.         1.         0.93442623]\n",
      "Mean CV accuracy (Gaussian Naive Bayes): 0.9804865150713908\n",
      "Testing Set Accuracy with cross-validation (Gaussian Naive Bayes): 0.9886363636363636\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9814814814814814\n",
      "F1 Score: 0.9865009970854426\n",
      "Recall: 0.9919354838709677\n",
      "Kappa Score: 0.9730061349693252\n",
      "Log Loss: 0.40958697033087693\n",
      "Matthews Correlation Coefficient: 0.9733608284033275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB  # Import Gaussian Naive Bayes\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.125, random_state=42)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training and testing\n",
    "pipeline_cv_gnb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_cv_gnb, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Gaussian Naive Bayes):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (Gaussian Naive Bayes):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_gnb\n",
    "pipeline_cv_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_gnb.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Gaussian Naive Bayes):\", test_accuracy_test_gnb)\n",
    "accuracy_qt_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_gnb.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f7480b4-e25a-4ec0-885b-a1d84a0242d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [0.96491228 0.98245614 0.98245614 0.91071429 0.94642857 0.96428571\n",
      " 0.96428571 0.98214286 1.         0.98214286]\n",
      "Mean CV accuracy (KNN): 0.9679824561403508\n",
      "Testing Set Accuracy with cross-validation (KNN): 0.9858156028368794\n",
      "printing precision\n",
      "0.9906542056074766\n",
      "f1-score\n",
      "0.9809973045822102\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9722222222222222\n",
      "kappa score\n",
      "0.9620048504446241\n",
      "log loss\n",
      "0.07998592530232557\n",
      "MCC\n",
      "0.9626999933824488\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance',algorithm='auto', p=3, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',]),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_cv_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_knn = cross_val_score(pipeline_cv_knn, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV accuracy (KNN):\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_knn\n",
    "pipeline_cv_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test_knn = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN):\", test_accuracy_test_knn)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_qt_knn=test_accuracy_test\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_knn.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c80c62eb-a31a-4ff6-b01f-d9f8a45a2592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\78262861.py:36: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data[attr][data[attr] == ''] = np.nan\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\78262861.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  categorical_df[col].fillna(mode_val, inplace=True)\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\78262861.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing dataset: 0.9929078014184397\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\autism+screening+adult\\\\Autism-Adult-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert dtype_mapping to list of tuples\n",
    "dtype_tuples = [(col, dtype_mapping[col]) for col in meta.names()]\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# # Print columns and their types\n",
    "# print(\"Columns and their types:\")\n",
    "# print(df.dtypes)\n",
    "# print('DataFrame:')\n",
    "# print(df)\n",
    "\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming categorical_df contains the one-hot encoded categorical columns\n",
    "# and non_categorical_df contains the bool and float columns\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "# Display final DataFrame\n",
    "# print(joined_df)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming joined_df contains the joined DataFrame\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "# accuracy_list={}\n",
    "# Split the data into training and testing sets\n",
    "# for i in range(1,101):\n",
    "    \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "    \n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "    \n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "    \n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "    \n",
    "# Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "    \n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', QuantileTransformer(n_quantiles=84,output_distribution='uniform',subsample=350, random_state=15)),\n",
    "    ('oversampler', RandomOverSampler(random_state=4)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='lsqr',shrinkage=0.25,priors=prior_probabilities, store_covariance=True, tol=0.00009))\n",
    "])\n",
    "    \n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "    \n",
    "# Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "    \n",
    "# Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy on testing dataset:\", accuracy)\n",
    "accuracy_qt_lda=accuracy\n",
    "# accuracy_list[i]=accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "901dd4b8-8610-4036-94d8-6ea02730833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Logistic Regression): [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.85714286 1.        ]\n",
      "Mean CV accuracy (Logistic Regression): 0.9857142857142858\n",
      "Testing Set Accuracy with cross-validation (Logistic Regression): 0.9889589905362776\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9905860412834662\n",
      "F1 Score: 0.985960450100757\n",
      "Recall: 0.9815743608390908\n",
      "Kappa Score: 0.9719231207217238\n",
      "Log Loss: 0.1179129880081373\n",
      "Matthews Correlation Coefficient: 0.9721186332288154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=37)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=1000)  # Adjust max_iter as needed\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training and testing\n",
    "pipeline_cv_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_cv_lr, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Logistic Regression):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (Logistic Regression):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_lr\n",
    "pipeline_cv_lr.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_lr.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Logistic Regression):\", test_accuracy_test_lr)\n",
    "accuracy_qt_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_lr.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c60a7994-b3be-4145-90e9-e08c8ef5d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (563). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value: 0.9858156028368794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'data.csv' is your dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "\n",
    "# Assuming 'Class/ASD Traits' is the column you want to predict\n",
    "X = data.iloc[:, :-1]  # Exclude the last column (target variable)\n",
    "y = data.iloc[:, -1]  # Target variable\n",
    "X = X.iloc[:, :-5]\n",
    "\n",
    "# Splitting data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Imputation for numerical columns (replace NaN values with mean)\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_train[numeric_columns] = numeric_imputer.fit_transform(X_train[numeric_columns])\n",
    "X_test[numeric_columns] = numeric_imputer.transform(X_test[numeric_columns])\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_train_encoded = pd.DataFrame(onehot_encoder.fit_transform(X_train[categorical_columns]))\n",
    "X_test_encoded = pd.DataFrame(onehot_encoder.transform(X_test[categorical_columns]))\n",
    "\n",
    "# Concatenating encoded features with original dataset\n",
    "X_train_encoded.index = X_train.index\n",
    "X_test_encoded.index = X_test.index\n",
    "X_train = pd.concat([X_train.drop(columns=categorical_columns), X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=categorical_columns), X_test_encoded], axis=1)\n",
    "\n",
    "# Convert feature names to string\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "# Scaling features using Quantile Transformer\n",
    "scaler = QuantileTransformer()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "# 10-fold Cross Validation\n",
    "cv_scores = cross_val_score(random_forest_classifier, X_train_scaled, y_train, cv=10)\n",
    "\n",
    "# Training\n",
    "random_forest_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Testing\n",
    "y_pred = random_forest_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy Value:\", accuracy)\n",
    "accuracy_qt_rf=accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b22ae97a-3164-4349-80d4-70961eda8465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Support Vector Machine): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy (Support Vector Machine): 1.0\n",
      "Testing Set Accuracy with cross-validation (Support Vector Machine): 0.9921135646687698\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9926959003632589\n",
      "F1 Score: 0.9897790402615535\n",
      "Recall: 0.9869532882842452\n",
      "Kappa Score: 0.9795586736996866\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC  # Import Support Vector Machine\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=35)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Support Vector Machine classifier for training and testing\n",
    "pipeline_cv_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_cv_svm, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Support Vector Machine):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (Support Vector Machine):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_svm\n",
    "pipeline_cv_svm.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_svm.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Support Vector Machine):\", test_accuracy_test_svm)\n",
    "accuracy_qt_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "# SVM does not provide predict_proba method, so you cannot compute log loss or ROC AUC\n",
    "# print(\"Log Loss:\", log_loss(y_test, pipeline_cv_svm.predict_proba(X_test)))\n",
    "# print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e5432c8-2d92-4918-a4dd-78dce2c56370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96226415 1.         1.         1.         1.         0.98113208\n",
      " 1.         1.         0.98076923 0.98076923]\n",
      "Mean CV accuracy: 0.9904934687953555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation: 0.9943181818181818\n",
      "printing precision\n",
      "0.996\n",
      "f1-score\n",
      "0.9931375989394471\n",
      "Testing Set Accuracy without cross-validation: 0.9943181818181818\n",
      "Testing Set Accuracy without cross-validation: 0.9943181818181818\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9903846153846154\n",
      "kappa score\n",
      "0.9862757330006239\n",
      "log loss\n",
      "0.05443085381557306\n",
      "MCC\n",
      "0.9863686313559838\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=35)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=32, learning_rate=0.1, algorithm='SAMME.R', random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21255437-79f2-49cf-b46c-c8890df9cb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree): [1.         0.98113208 0.98113208 0.96226415 0.98113208 1.\n",
      " 1.         0.94339623 1.         1.        ]\n",
      "Mean CV accuracy (Decision Tree): 0.9849056603773585\n",
      "Testing Set Accuracy with cross-validation (Decision Tree): 0.9772727272727273\n",
      "printing precision\n",
      "0.9717178209866624\n",
      "f1-score\n",
      "0.9717178209866624\n",
      "Testing Set Accuracy without cross-validation: 0.9772727272727273\n",
      "Testing Set Accuracy without cross-validation: 0.9772727272727273\n",
      "ROC AUC: 0.9717178209866624\n",
      "recall\n",
      "0.9717178209866624\n",
      "kappa score\n",
      "0.9434356419733247\n",
      "log loss\n",
      "0.8191739406617536\n",
      "MCC\n",
      "0.9434356419733247\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini',splitter='random',max_depth=None, min_samples_split=3,random_state=39)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))  # Using PowerTransformer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_dt = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree):\", cv_scores_dt)\n",
    "print(\"Mean CV accuracy (Decision Tree):\", np.mean(cv_scores_dt))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_dt\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fdc9200-f4d1-44e9-9a36-088d7deef904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Gaussian Naive Bayes): [0.96226415 0.98113208 0.98113208 1.         0.94339623 1.\n",
      " 1.         0.96226415 0.98076923 0.94230769]\n",
      "Mean CV accuracy (Gaussian Naive Bayes): 0.9753265602322205\n",
      "Testing Set Accuracy with cross-validation (Gaussian Naive Bayes): 0.9829545454545454\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9711538461538461\n",
      "F1 Score: 0.9791724192339553\n",
      "Recall: 0.9881889763779528\n",
      "Kappa Score: 0.9583596214511041\n",
      "Log Loss: 0.6143804554963153\n",
      "Matthews Correlation Coefficient: 0.9591915634958048\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson'))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training and testing\n",
    "pipeline_cv_gnb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_cv_gnb, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Gaussian Naive Bayes):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (Gaussian Naive Bayes):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_gnb\n",
    "pipeline_cv_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_gnb.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Gaussian Naive Bayes):\", test_accuracy_test_gnb)\n",
    "accuracy_pt_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_gnb.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26343bea-1ec9-41a7-88f6-333ddaf5b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [0.98076923 0.98076923 0.98076923 0.98076923 0.96153846 1.\n",
      " 1.         0.96078431 1.         0.96078431]\n",
      "Mean CV accuracy (KNN): 0.9806184012066366\n",
      "Testing Set Accuracy with cross-validation (KNN): 0.9893048128342246\n",
      "printing precision\n",
      "0.9927536231884058\n",
      "f1-score\n",
      "0.9863503649635037\n",
      "Testing Set Accuracy without cross-validation: 0.9893048128342246\n",
      "Testing Set Accuracy without cross-validation: 0.9893048128342246\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9803921568627452\n",
      "kappa score\n",
      "0.9727047146401985\n",
      "log loss\n",
      "0.050141567858778886\n",
      "MCC\n",
      "0.9730672655996823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.265, random_state=43)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance', algorithm='auto', p=3, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))  # Using PowerTransformer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_cv_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_knn = cross_val_score(pipeline_cv_knn, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV accuracy (KNN):\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_knn\n",
    "pipeline_cv_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test_knn = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN):\", test_accuracy_test_knn)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_knn=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_knn.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebd56dbe-2c6d-4a87-a1ce-d37cf8551d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\2913260596.py:43: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data[attr][data[attr] == ''] = np.nan\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\2913260596.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  categorical_df[col].fillna(mode_val, inplace=True)\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\2913260596.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing dataset: 0.9361702127659575\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\autism+screening+adult\\\\Autism-Adult-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert dtype_mapping to list of tuples\n",
    "dtype_tuples = [(col, dtype_mapping[col]) for col in meta.names()]\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "\n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "\n",
    "# Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', PowerTransformer()),  # Replace QuantileTransformer with PowerTransformer\n",
    "    ('oversampler', RandomOverSampler(random_state=4)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='lsqr', shrinkage=0.25, priors=prior_probabilities, store_covariance=True, tol=0.00009))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy on testing dataset:\", accuracy)\n",
    "accuracy_pt_lda=accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e24fdf5-6160-4036-aedb-4bd66b0381a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Logistic Regression): [1.         0.85714286 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy (Logistic Regression): 0.9857142857142858\n",
      "Testing Set Accuracy with cross-validation (Logistic Regression): 0.9889589905362776\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9905776953969725\n",
      "F1 Score: 0.9858548448272015\n",
      "Recall: 0.9813762267439657\n",
      "Kappa Score: 0.9717119437043459\n",
      "Log Loss: 0.06643238124127139\n",
      "Matthews Correlation Coefficient: 0.9719103661036755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=43)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson'))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training and testing\n",
    "pipeline_cv_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_cv_lr, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Logistic Regression):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (Logistic Regression):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_lr\n",
    "pipeline_cv_lr.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lr = pipeline_cv_lr.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(\"Testing Set Accuracy with cross-validation (Logistic Regression):\", test_accuracy_test_lr)\n",
    "accuracy_pt_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lr))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_lr.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea25df26-2d39-43b0-b8dc-c124db8cb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.86764706 0.75       0.83823529 0.88235294 0.82352941 0.85294118\n",
      " 0.83823529 0.88235294 0.74626866 0.7761194 ]\n",
      "Mean CV accuracy: 0.8257682177348551\n",
      "Testing Set Accuracy with cross-validation: 0.9615384615384616\n",
      "printing precision\n",
      "0.9761904761904762\n",
      "f1-score\n",
      "0.9423503325942351\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9166666666666667\n",
      "kappa score\n",
      "0.8849557522123894\n",
      "log loss\n",
      "0.31191804715506427\n",
      "MCC\n",
      "0.8908708063747479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.036, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=False))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_pt_rf=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae49859-3296-4ab3-9667-799c07be7daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Support Vector Machine): [1.         1.         1.         1.         1.         1.\n",
      " 1.         0.85714286 0.85714286 1.        ]\n",
      "Mean CV accuracy (Support Vector Machine): 0.9714285714285715\n",
      "Testing Set Accuracy with cross-validation (Support Vector Machine): 0.9794952681388013\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9863445378151261\n",
      "F1 Score: 0.9733209033732451\n",
      "Recall: 0.9619883040935673\n",
      "Kappa Score: 0.9466709802652863\n",
      "Matthews Correlation Coefficient: 0.9480200171523382\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC  # Import Support Vector Classifier (SVM)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=39)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson'))\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Support Vector Machine classifier for training and testing\n",
    "pipeline_cv_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_cv_svm, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Support Vector Machine):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (Support Vector Machine):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_svm\n",
    "pipeline_cv_svm.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_svm = pipeline_cv_svm.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(\"Testing Set Accuracy with cross-validation (Support Vector Machine):\", test_accuracy_test_svm)\n",
    "accuracy_pt_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_svm))\n",
    "# As SVM doesn't have predict_proba method, you can't compute log loss for it\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e72f680-e498-4c30-99eb-b75d53a501e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.96226415 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy: 0.9962264150943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation: 0.9886363636363636\n",
      "printing precision\n",
      "0.9818181818181818\n",
      "f1-score\n",
      "0.986642380085003\n",
      "Testing Set Accuracy without cross-validation: 0.9886363636363636\n",
      "Testing Set Accuracy without cross-validation: 0.9886363636363636\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9918699186991871\n",
      "kappa score\n",
      "0.9732888146911519\n",
      "log loss\n",
      "0.044662076069365376\n",
      "MCC\n",
      "0.9736362152646113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=39)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=32, learning_rate=0.1, algorithm='SAMME.R', random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Using Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_ab=test_accuracy_test\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa8df4b9-e407-4664-9263-9f2a1ae99789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree): [1.         0.98       1.         0.95918367 1.         0.97959184\n",
      " 1.         0.95918367 1.         1.        ]\n",
      "Mean CV accuracy (Decision Tree): 0.9877959183673468\n",
      "Testing Set Accuracy with cross-validation (Decision Tree): 0.9811320754716981\n",
      "Precision: 0.9760045274476514\n",
      "F1 Score: 0.9760045274476514\n",
      "Recall: 0.9760045274476514\n",
      "Cohen's Kappa Score: 0.9520090548953027\n",
      "Log Loss: 0.6800689318701351\n",
      "Matthews Correlation Coefficient: 0.9520090548953027\n",
      "ROC AUC: 0.9760045274476512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', splitter='random', max_depth=None, min_samples_split=3, random_state=39)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Using Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_dt = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree):\", cv_scores_dt)\n",
    "print(\"Mean CV accuracy (Decision Tree):\", np.mean(cv_scores_dt))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_dt\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_dt = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test_dt)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree):\", test_accuracy_test_dt)\n",
    "accuracy_nor_dt=test_accuracy_test_dt\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_dt, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test_dt, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_dt, average='macro'))\n",
    "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_dt))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_dt))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, pipeline_cv_dt.predict_proba(X_test)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "723860af-b9a7-4ae0-a9b0-826d25853550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Gaussian Naive Bayes): [0.96491228 1.         0.98245614 0.94642857 0.98214286 0.98214286\n",
      " 0.98214286 0.98214286 0.96428571 0.98214286]\n",
      "Mean CV accuracy (Gaussian Naive Bayes): 0.9768796992481203\n",
      "Testing Set Accuracy with cross-validation (Gaussian Naive Bayes): 0.9787234042553191\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9615384615384616\n",
      "F1 Score: 0.9727536231884057\n",
      "Recall: 0.9857142857142858\n",
      "Kappa Score: 0.9455388180764774\n",
      "Log Loss: 0.7668862423216417\n",
      "Matthews Correlation Coefficient: 0.9469441888006371\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB  # Import Gaussian Naive Bayes\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training and testing\n",
    "pipeline_cv_gnb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_cv_gnb, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Gaussian Naive Bayes):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (Gaussian Naive Bayes):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_gnb\n",
    "pipeline_cv_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_gnb.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Gaussian Naive Bayes):\", test_accuracy_test_gnb)\n",
    "accuracy_nor_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_gnb.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a77fdbd-294c-48ff-9b39-b30c3566e8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN): [0.94230769 0.98076923 0.96153846 0.98076923 0.98076923 0.92307692\n",
      " 0.92307692 0.98039216 0.90196078 0.94117647]\n",
      "Mean CV accuracy (KNN): 0.9515837104072398\n",
      "Testing Set Accuracy with cross-validation (KNN): 0.9732620320855615\n",
      "printing precision\n",
      "0.967622571692877\n",
      "f1-score\n",
      "0.9642214824168676\n",
      "Testing Set Accuracy without cross-validation: 0.9732620320855615\n",
      "Testing Set Accuracy without cross-validation: 0.9732620320855615\n",
      "ROC AUC: 0.9916413373860182\n",
      "recall\n",
      "0.9609422492401216\n",
      "kappa score\n",
      "0.9284457029157419\n",
      "log loss\n",
      "0.12330287260195638\n",
      "MCC\n",
      "0.9285407906851798\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.265, random_state=32)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance', algorithm='auto', p=3, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Using Normalizer\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_cv_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_knn = cross_val_score(pipeline_cv_knn, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN):\", cv_scores_knn)\n",
    "print(\"Mean CV accuracy (KNN):\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_knn\n",
    "pipeline_cv_knn.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test_knn = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN):\", test_accuracy_test_knn)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_knn.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_knn.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_knn=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_knn.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70d0f565-b017-414e-b5d4-91939e70960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (LDA): [0.79310345 0.89285714 0.71428571 0.78571429 0.67857143 0.71428571\n",
      " 0.89285714 0.82142857 0.75       0.75      ]\n",
      "Mean CV accuracy (LDA): 0.7793103448275862\n",
      "Testing Set Accuracy with cross-validation (LDA): 0.7825059101654847\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.7349707165824528\n",
      "F1 Score: 0.7282554047259929\n",
      "Recall: 0.7228772942650182\n",
      "Kappa Score: 0.45689005498646273\n",
      "Matthews Correlation Coefficient: 0.4576882674633885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Import LDA classifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.6, random_state=44)\n",
    "\n",
    "# Define LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training and testing\n",
    "pipeline_cv_lda = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lda)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lda = cross_val_score(pipeline_cv_lda, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (LDA):\", cv_scores_lda)\n",
    "print(\"Mean CV accuracy (LDA):\", np.mean(cv_scores_lda))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_lda\n",
    "pipeline_cv_lda.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lda = pipeline_cv_lda.predict(X_test)\n",
    "test_accuracy_test_lda = accuracy_score(y_test, y_pred_test_lda)\n",
    "print(\"Testing Set Accuracy with cross-validation (LDA):\", test_accuracy_test_lda)\n",
    "accuracy_nor_lda=test_accuracy_test_lda\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lda, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lda))\n",
    "# LDA does not have a predict_proba method, so log_loss cannot be computed\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b33f6a17-afc7-4815-b488-acd951aeaa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Logistic Regression): [1.         0.92857143 1.         1.         1.         1.\n",
      " 0.92857143 1.         1.         1.        ]\n",
      "Mean CV accuracy (Logistic Regression): 0.9857142857142858\n",
      "Testing Set Accuracy with cross-validation (Logistic Regression): 0.9946808510638298\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9963942307692308\n",
      "F1 Score: 0.9931738686655558\n",
      "Recall: 0.990066225165563\n",
      "Kappa Score: 0.9863482330159755\n",
      "Log Loss: 0.09379975363954283\n",
      "Matthews Correlation Coefficient: 0.9864401590913465\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.8, random_state=44)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training and testing\n",
    "pipeline_cv_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_cv_lr, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Logistic Regression):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (Logistic Regression):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_lr\n",
    "pipeline_cv_lr.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_lr = pipeline_cv_lr.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(\"Testing Set Accuracy with cross-validation (Logistic Regression):\", test_accuracy_test_lr)\n",
    "accuracy_nor_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_lr, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_lr))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_lr.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8563114c-73e8-46df-8298-0cb2a484cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.89705882 0.80882353 0.86764706 0.86764706 0.83823529 0.89705882\n",
      " 0.83823529 0.86764706 0.79104478 0.79104478]\n",
      "Mean CV accuracy: 0.8464442493415276\n",
      "Testing Set Accuracy with cross-validation: 0.9615384615384616\n",
      "printing precision\n",
      "0.9761904761904762\n",
      "f1-score\n",
      "0.9423503325942351\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9166666666666667\n",
      "kappa score\n",
      "0.8849557522123894\n",
      "log loss\n",
      "0.29820935995496844\n",
      "MCC\n",
      "0.8908708063747479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.036, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_rf=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38f9cc0b-1590-408e-a0bd-89d82f48b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (SVM): [0.78571429 1.         1.         1.         0.85714286 1.\n",
      " 1.         1.         0.92857143 0.85714286]\n",
      "Mean CV accuracy (SVM): 0.9428571428571428\n",
      "Testing Set Accuracy with cross-validation (SVM): 0.9627659574468085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.976027397260274\n",
      "F1 Score: 0.9492577597840756\n",
      "Recall: 0.9285714285714286\n",
      "Kappa Score: 0.8987068965517242\n",
      "Matthews Correlation Coefficient: 0.9033531793998947\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC  # Import SVM classifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.8, random_state=69)\n",
    "\n",
    "# Define SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', Normalizer())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training and testing\n",
    "pipeline_cv_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_cv_svm, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (SVM):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (SVM):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_svm\n",
    "pipeline_cv_svm.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_svm = pipeline_cv_svm.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(\"Testing Set Accuracy with cross-validation (SVM):\", test_accuracy_test_svm)\n",
    "accuracy_nor_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test_svm, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test_svm))\n",
    "# SVM does not have a predict_proba method, so log_loss cannot be computed\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64ca01a5-5c4a-4a06-838d-afd3b0308acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (AdaBoost with MaxAbsScaler): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy (AdaBoost with MaxAbsScaler): 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy with cross-validation (AdaBoost with MaxAbsScaler): 0.9891304347826086\n",
      "printing precision\n",
      "0.9787234042553192\n",
      "f1-score\n",
      "0.9855072463768115\n",
      "Testing Set Accuracy without cross-validation: 0.9891304347826086\n",
      "Testing Set Accuracy without cross-validation: 0.9891304347826086\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9928057553956835\n",
      "kappa score\n",
      "0.9710190581193889\n",
      "log loss\n",
      "0.03862235994167358\n",
      "MCC\n",
      "0.97142709218888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.26, random_state=32)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, min_samples_split=2), n_estimators=100, learning_rate=0.1, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training and testing\n",
    "pipeline_cv_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (AdaBoost with MaxAbsScaler):\", cv_scores)\n",
    "print(\"Mean CV accuracy (AdaBoost with MaxAbsScaler):\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test= pipeline_cv_mas.predict(X_test)\n",
    "test_accuracy_test_mas = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (AdaBoost with MaxAbsScaler):\", test_accuracy_test_mas)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_mas.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88b1012f-db34-4df0-81d9-09ed6ee0640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Decision Tree with MaxAbsScaler): [0.96491228 1.         0.98245614 1.         0.98214286 1.\n",
      " 1.         1.         1.         1.        ]\n",
      "Mean CV accuracy (Decision Tree with MaxAbsScaler): 0.9929511278195488\n",
      "Testing Set Accuracy with cross-validation (Decision Tree with MaxAbsScaler): 0.9858156028368794\n",
      "printing precision\n",
      "0.975\n",
      "f1-score\n",
      "0.9822775263951735\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "ROC AUC: 0.99386816555953\n",
      "recall\n",
      "0.9902912621359223\n",
      "kappa score\n",
      "0.9645639607941694\n",
      "log loss\n",
      "0.2756621142730211\n",
      "MCC\n",
      "0.9651701394356605\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=45)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=5, min_samples_split=2, random_state=57)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training and testing\n",
    "pipeline_cv_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv_dt, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Decision Tree with MaxAbsScaler):\", cv_scores)\n",
    "print(\"Mean CV accuracy (Decision Tree with MaxAbsScaler):\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv_dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test_dt = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Decision Tree with MaxAbsScaler):\", test_accuracy_test_dt)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv_dt.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv_dt.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv_dt.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bf7222c-0785-44d7-9708-875c0d6cfced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Gaussian Naive Bayes): [0.98387097 0.96774194 0.96774194 1.         1.         0.98387097\n",
      " 0.96721311 1.         1.         0.93442623]\n",
      "Mean CV accuracy (Gaussian Naive Bayes): 0.9804865150713908\n",
      "Testing Set Accuracy with cross-validation (Gaussian Naive Bayes): 0.9886363636363636\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9814814814814814\n",
      "F1 Score: 0.9865009970854426\n",
      "Recall: 0.9919354838709677\n",
      "Kappa Score: 0.9730061349693252\n",
      "Log Loss: 0.40958697033087693\n",
      "Matthews Correlation Coefficient: 0.9733608284033275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import GaussianNB  # Import Gaussian Naive Bayes\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.125, random_state=42)\n",
    "\n",
    "# Define Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training and testing\n",
    "pipeline_cv_gnb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', gnb)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_gnb = cross_val_score(pipeline_cv_gnb, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Gaussian Naive Bayes):\", cv_scores_gnb)\n",
    "print(\"Mean CV accuracy (Gaussian Naive Bayes):\", np.mean(cv_scores_gnb))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_gnb\n",
    "pipeline_cv_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_gnb.predict(X_test)\n",
    "test_accuracy_test_gnb = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Gaussian Naive Bayes):\", test_accuracy_test_gnb)\n",
    "accuracy_mas_gnb=test_accuracy_test_gnb\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_gnb.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfbf8648-3820-404c-9fdb-f0049d396171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (KNN with MaxAbsScaler): [0.96491228 0.98245614 0.98245614 0.91071429 0.94642857 0.96428571\n",
      " 0.96428571 0.98214286 1.         0.98214286]\n",
      "Mean CV accuracy (KNN with MaxAbsScaler): 0.9679824561403508\n",
      "Testing Set Accuracy with cross-validation (KNN with MaxAbsScaler): 0.9858156028368794\n",
      "printing precision\n",
      "0.9906542056074766\n",
      "f1-score\n",
      "0.9809973045822102\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "Testing Set Accuracy without cross-validation: 0.9858156028368794\n",
      "ROC AUC: 0.9989417989417989\n",
      "recall\n",
      "0.9722222222222222\n",
      "kappa score\n",
      "0.9620048504446241\n",
      "log loss\n",
      "0.08245994524118187\n",
      "MCC\n",
      "0.9626999933824488\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training and testing\n",
    "pipeline_knn_mas = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_knn_mas, X_train, y_train, cv=cv)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (KNN with MaxAbsScaler):\", cv_scores)\n",
    "print(\"Mean CV accuracy (KNN with MaxAbsScaler):\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_knn_mas.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_knn_mas.predict(X_test)\n",
    "test_accuracy_test_knn_mas = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (KNN with MaxAbsScaler):\", test_accuracy_test_knn_mas)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_knn_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_knn_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_knn_mas.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_knn_mas.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_knn=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_knn_mas.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eeba3656-4292-4e24-bf7e-b55b9bf7a4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\2029962674.py:43: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data[attr][data[attr] == ''] = np.nan\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\2029962674.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  categorical_df[col].fillna(mode_val, inplace=True)\n",
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_12224\\2029962674.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing dataset: 0.9880239520958084\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\autism+screening+adult\\\\Autism-Adult-Data.arff\")\n",
    "\n",
    "# Define data types mapping\n",
    "dtype_mapping = {\n",
    "    'A1_Score': 'bool',\n",
    "    'A2_Score': 'bool',\n",
    "    'A3_Score': 'bool',\n",
    "    'A4_Score': 'bool',\n",
    "    'A5_Score': 'bool',\n",
    "    'A6_Score': 'bool',\n",
    "    'A7_Score': 'bool',\n",
    "    'A8_Score': 'bool',\n",
    "    'A9_Score': 'bool',\n",
    "    'A10_Score': 'bool',\n",
    "    'age': 'float',\n",
    "    'gender': 'str',\n",
    "    'ethnicity': 'str',\n",
    "    'jundice': 'bool',\n",
    "    'austim': 'bool',\n",
    "    'contry_of_res': 'str',\n",
    "    'used_app_before': 'bool',\n",
    "    'result': 'float',\n",
    "    'age_desc': 'str',\n",
    "    'relation': 'str',\n",
    "    'Class/ASD': 'str'  # Assuming this is your target variable\n",
    "}\n",
    "\n",
    "# Replace missing value symbols ('?' or '') with NaN\n",
    "for attr in meta.names():\n",
    "    data[attr] = np.char.strip(np.char.mod('%s', data[attr].astype(str)))\n",
    "    data[attr][data[attr] == ''] = np.nan\n",
    "\n",
    "# Convert nominal attributes to strings\n",
    "for attr in meta.names():\n",
    "    if meta[attr][0] == 'nominal':\n",
    "        data[attr] = data[attr].astype(str)\n",
    "\n",
    "# Convert dtype_mapping to list of tuples\n",
    "dtype_tuples = [(col, dtype_mapping[col]) for col in meta.names()]\n",
    "\n",
    "# Convert to DataFrame with specified data types\n",
    "df = pd.DataFrame(data, columns=meta.names())\n",
    "\n",
    "# Apply the specified data types\n",
    "df = df.astype(dtype_mapping)\n",
    "\n",
    "# Separate columns with nominal values into categorical_df\n",
    "nominal_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_df = df[nominal_columns]\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "for col in categorical_df.columns:\n",
    "    mode_val = categorical_df[col].mode()[0]\n",
    "    categorical_df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Separate remaining columns into non_categorical_df\n",
    "non_categorical_columns = [col for col in df.columns if col not in nominal_columns]\n",
    "non_categorical_df = df[non_categorical_columns]\n",
    "\n",
    "# Check for missing values in columns with bool values\n",
    "bool_columns_with_missing = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'bool' and non_categorical_df[col].isnull().any()]\n",
    "if bool_columns_with_missing:\n",
    "    print(\"Missing values found in columns with bool values. Cannot proceed with mean value imputation.\")\n",
    "else:\n",
    "    # Apply mean value imputation to float columns in non_categorical_df\n",
    "    float_columns = [col for col in non_categorical_df.columns if non_categorical_df[col].dtype == 'float64']\n",
    "    non_categorical_df[float_columns] = non_categorical_df[float_columns].fillna(non_categorical_df[float_columns].mean())\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_data = encoder.fit_transform(categorical_df)\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_df.columns))\n",
    "\n",
    "# Join encoded categorical columns with bool and float columns\n",
    "joined_df = pd.concat([non_categorical_df, encoded_df], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = joined_df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = joined_df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.237, random_state=15)\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode string labels into numerical values\n",
    "labels_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Now, you can calculate class counts\n",
    "class_counts = np.bincount(labels_encoded)\n",
    "\n",
    "# Calculate prior probabilities based on class proportions\n",
    "prior_probabilities = class_counts / len(labels_encoded)\n",
    "\n",
    "# Fit Gaussian distributions to the prior probabilities\n",
    "means = np.mean(prior_probabilities, axis=0)  # Calculate mean for each class\n",
    "variances = np.var(prior_probabilities, axis=0)  # Calculate variance for each class\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', MaxAbsScaler()),  # Replace QuantileTransformer with MaxAbsScaler\n",
    "    ('oversampler', RandomOverSampler(random_state=4)),\n",
    "    ('classifier', LinearDiscriminantAnalysis(solver='lsqr', shrinkage=0.25, priors=prior_probabilities, store_covariance=True, tol=0.00009))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Perform 10-fold cross-validation on training data\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy on testing dataset:\", accuracy)\n",
    "accuracy_mas_lda=accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38d4de7b-15bd-41e2-a42a-4f469c653786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Logistic Regression): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CV accuracy (Logistic Regression): 1.0\n",
      "Testing Set Accuracy with cross-validation (Logistic Regression): 0.9936908517350158\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9956989247311828\n",
      "F1 Score: 0.9919922195698028\n",
      "Recall: 0.9884393063583815\n",
      "Kappa Score: 0.9839852482412822\n",
      "Log Loss: 0.11499281540892077\n",
      "Matthews Correlation Coefficient: 0.9841114549852461\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=41)\n",
    "\n",
    "# Define Logistic Regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training and testing\n",
    "pipeline_cv_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_lr = cross_val_score(pipeline_cv_lr, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Logistic Regression):\", cv_scores_lr)\n",
    "print(\"Mean CV accuracy (Logistic Regression):\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_lr\n",
    "pipeline_cv_lr.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_lr.predict(X_test)\n",
    "test_accuracy_test_lr = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Logistic Regression):\", test_accuracy_test_lr)\n",
    "accuracy_mas_lr=test_accuracy_test_lr\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Log Loss:\", log_loss(y_test, pipeline_cv_lr.predict_proba(X_test)))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7056570e-c98f-4ba6-8387-401bbec61f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.86764706 0.75       0.83823529 0.88235294 0.82352941 0.85294118\n",
      " 0.83823529 0.88235294 0.74626866 0.7761194 ]\n",
      "Mean CV accuracy: 0.8257682177348551\n",
      "Testing Set Accuracy with cross-validation: 0.9615384615384616\n",
      "printing precision\n",
      "0.9761904761904762\n",
      "f1-score\n",
      "0.9423503325942351\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "Testing Set Accuracy without cross-validation: 0.9615384615384616\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9166666666666667\n",
      "kappa score\n",
      "0.8849557522123894\n",
      "log loss\n",
      "0.31191804715506427\n",
      "MCC\n",
      "0.8908708063747479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.036, random_state=43)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('scaler_mas', MaxAbsScaler())  # Replaced Normalizer with MaxAbsScaler\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training and testing\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_rf=test_accuracy_test\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11eb4248-3aa1-481a-9e87-2ec5f700011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Support Vector Machine): [1.         1.         1.         0.85714286 1.         1.\n",
      " 1.         0.85714286 0.71428571 0.85714286]\n",
      "Mean CV accuracy (Support Vector Machine): 0.9285714285714286\n",
      "Testing Set Accuracy with cross-validation (Support Vector Machine): 0.9889589905362776\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.9905776953969725\n",
      "F1 Score: 0.9858548448272015\n",
      "Recall: 0.9813762267439657\n",
      "Kappa Score: 0.9717119437043459\n",
      "Matthews Correlation Coefficient: 0.9719103661036755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC  # Import Support Vector Classifier (SVM)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\csv_result-Autism-Adult-Data.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [10, 11, 12, 13, 14, 15, 16,17, 18, 19]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-10:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.9, random_state=30)\n",
    "\n",
    "# Define Support Vector Machine classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Support Vector Machine classifier for training and testing\n",
    "pipeline_cv_svm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', svm)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores_svm = cross_val_score(pipeline_cv_svm, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores (Support Vector Machine):\", cv_scores_svm)\n",
    "print(\"Mean CV accuracy (Support Vector Machine):\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Train the model on the entire training set using pipeline_cv_svm\n",
    "pipeline_cv_svm.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_cv_svm.predict(X_test)\n",
    "test_accuracy_test_svm = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with cross-validation (Support Vector Machine):\", test_accuracy_test_svm)\n",
    "accuracy_mas_svm=test_accuracy_test_svm\n",
    "for i in range(5):\n",
    "    print()\n",
    "# Calculate and print additional evaluation metrics\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a5d6b77-c516-4628-af50-cdf15332ce37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "transformers = ['qt', 'pt','nor','mas']\n",
    "models = ['ab', 'rf', 'dt', 'knn', 'gnb', 'lr', 'svm', 'lda']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collecting accuracies for each transformer\n",
    "accuracies = {}\n",
    "for transformer in transformers:\n",
    "    accuracies[transformer] = [globals()[f\"accuracy_{transformer}_{model}\"] for model in models]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for transformer in transformers:\n",
    "    ax.plot(models, accuracies[transformer], label=transformer)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of Models for Different Transformers')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ea389-422a-4b31-b652-aef3bad5ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
