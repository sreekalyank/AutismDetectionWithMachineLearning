{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a196e6f1-c149-4c97-b42c-f1875909d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELIEF ATTRIBUTE EVALUATOR\n",
      "Feature Importance Scores:\n",
      "A9: 0.0912\n",
      "A6: 0.0984\n",
      "A5: 0.0972\n",
      "A7: 0.0714\n",
      "A4: 0.1090\n",
      "A1: 0.0903\n",
      "A2: 0.0882\n",
      "A8: 0.1080\n",
      "A3: 0.0596\n",
      "Ethnicity_asian: 0.0045\n",
      "A10: 0.0872\n",
      "Ethnicity_middle eastern: 0.0025\n",
      "Jaundice_no: 0.0101\n",
      "Jaundice_yes: 0.0020\n",
      "Age_Mons: 0.0375\n",
      "Sex_m: 0.0061\n",
      "Sex_f: 0.0000\n",
      "Ethnicity_White European: 0.0000\n",
      "Ethnicity_south asian: 0.0033\n",
      "Family_mem_with_ASD_no: 0.0030\n",
      "Family_mem_with_ASD_yes: 0.0111\n",
      "Ethnicity_Hispanic: 0.0019\n",
      "Ethnicity_mixed: 0.0023\n",
      "Ethnicity_Latino: 0.0053\n",
      "Who completed the test_Health Care Professional: 0.0000\n",
      "Ethnicity_Native Indian: 0.0000\n",
      "Who completed the test_Others: 0.0000\n",
      "Who completed the test_Self: 0.0000\n",
      "Who completed the test_Health care professional: 0.0000\n",
      "Who completed the test_family member: 0.0000\n",
      "Ethnicity_Pacifica: 0.0000\n",
      "Ethnicity_black: 0.0038\n",
      "Ethnicity_Others: 0.0064\n",
      "Testing Set Accuracy: 0.9900497512437811\n"
     ]
    }
   ],
   "source": [
    "print(\"RELIEF ATTRIBUTE EVALUATOR\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skrebate import ReliefF\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.19, random_state=42)\n",
    "\n",
    "# ReliefF feature selection\n",
    "num_features_to_select = 10  # You can adjust this value\n",
    "# Initialize the ReliefF feature selector\n",
    "relief_selector = ReliefF(n_features_to_select=num_features_to_select)\n",
    "\n",
    "# Fit ReliefF selector on training data\n",
    "relief_selector.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "# Get selected features\n",
    "selected_features_indices = relief_selector.top_features_\n",
    "selected_features_names = result_df.columns[selected_features_indices]\n",
    "\n",
    "# Filter selected features in the dataset\n",
    "X_train_selected = X_train[selected_features_names]\n",
    "X_test_selected = X_test[selected_features_names]\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),\n",
    "                               n_estimators=100, learning_rate=0.5, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Fit AdaBoost classifier\n",
    "ada_boost.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the feature importance scores\n",
    "print(\"Feature Importance Scores:\")\n",
    "for feature, importance_score in zip(selected_features_names, ada_boost.feature_importances_):\n",
    "    print(f\"{feature}: {importance_score:.4f}\")\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = ada_boost.predict(X_test_selected)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1220facb-2f03-4501-b090-ccae605e149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATION ATTRIBUTE EVALUATOR\n",
      "Summed Feature Importance Scores for Categorical Attributes:\n",
      "Sex: 0.0000\n",
      "Ethnicity: 0.0000\n",
      "Jaundice: 0.0000\n",
      "Family_mem_with_ASD: 0.0000\n",
      "Who completed the test: 0.0000\n",
      "Feature Importance Scores:\n",
      "A1: 0.1080\n",
      "A2: 0.0919\n",
      "A3: 0.0714\n",
      "A4: 0.1152\n",
      "A5: 0.1055\n",
      "A6: 0.1066\n",
      "A7: 0.0940\n",
      "A8: 0.1113\n",
      "A9: 0.0866\n",
      "A10: 0.1096\n",
      "Testing Set Accuracy: 0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "print(\"CORRELATION ATTRIBUTE EVALUATOR\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.19, random_state=42)\n",
    "\n",
    "# Correlation-based feature selection\n",
    "num_features_to_select = 10  # You can adjust this value\n",
    "selector = SelectKBest(score_func=f_classif, k=num_features_to_select)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "selected_features_names = result_df.columns[selected_features_indices]\n",
    "\n",
    "# Filter selected features in the dataset\n",
    "X_train_selected = X_train[selected_features_names]\n",
    "X_test_selected = X_test[selected_features_names]\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),\n",
    "                               n_estimators=100, learning_rate=0.5, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Fit AdaBoost classifier\n",
    "ada_boost.fit(X_train_selected, y_train)\n",
    "\n",
    "feature_importance_scores = ada_boost.feature_importances_\n",
    "\n",
    "\n",
    "# Sum the feature importance scores for categorical attributes\n",
    "categorical_columns = category_features.columns\n",
    "categorical_importance_scores = {}\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    indices = [j for j in range(10) if col in X_train.columns[j]]\n",
    "    categorical_importance_scores[col] = np.sum(feature_importance_scores[indices])\n",
    "\n",
    "# Print the summed feature importance scores for categorical attributes\n",
    "print(\"Summed Feature Importance Scores for Categorical Attributes:\")\n",
    "for col, importance_score in categorical_importance_scores.items():\n",
    "    print(f\"{col}: {importance_score:.4f}\")\n",
    "\n",
    "\n",
    "# Print the feature importance scores\n",
    "print(\"Feature Importance Scores:\")\n",
    "for feature, importance_score in zip(selected_features_names, ada_boost.feature_importances_):\n",
    "    print(f\"{feature}: {importance_score:.4f}\")\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = ada_boost.predict(X_test_selected)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c375175-9fca-4c2e-b3be-91505c4afac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('INFO GAIN ATTRIBUTE EVALUATOR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "# Train AdaBoost classifier without feature selection for obtaining feature importances\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = np.mean([\n",
    "    tree.feature_importances_ for tree in ada_boost.estimators_\n",
    "], axis=0)\n",
    "\n",
    "# Sum feature importances for each categorical column\n",
    "categorical_importances = {}\n",
    "for col in category_features.columns:\n",
    "    cat_mask = result_df.columns.str.startswith(col)\n",
    "    cat_importance = np.sum(feature_importances[cat_mask])\n",
    "    categorical_importances[col] = cat_importance\n",
    "\n",
    "# Print feature importances for each categorical column\n",
    "print(\"\\nFeature Importances for Categorical Columns:\")\n",
    "for col, importance in categorical_importances.items():\n",
    "    print(f\"{col}: {importance}\")\n",
    "\n",
    "# Sum feature importances for numerical columns\n",
    "numerical_importances = {}\n",
    "for col in ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']:\n",
    "    num_importance = np.sum(feature_importances[result_df.columns == col])\n",
    "    numerical_importances[col] = num_importance\n",
    "\n",
    "# Print feature importances for each numerical column\n",
    "print(\"\\nFeature Importances for Numerical Columns:\")\n",
    "for col, importance in numerical_importances.items():\n",
    "    print(f\"{col}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d09b8-8416-4572-804a-61e9954d0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GAIN RATIO ATTRIBUTE EVALUATOR -  not working')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"D:\\\\Sem 6\\\\Mini Project\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns from original features\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, pd.DataFrame(category_transformed, columns=category_encoded_columns)], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.19, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5, min_samples_split=3, criterion='entropy')\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=300, learning_rate=0.1, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), category_encoded_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__learning_rate': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = grid_search.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy_test)\n",
    "\n",
    "# Calculate precision and F1 score\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Retrieve the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Calculate and print feature importances using Gain Ratio Attribute Evaluator\n",
    "# Get base estimator (decision tree) from AdaBoost classifier\n",
    "base_estimator = best_model.named_steps['classifier'].estimators_[0]\n",
    "\n",
    "# Calculate feature importances\n",
    "if hasattr(base_estimator, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    feature_importances = base_estimator.feature_importances_\n",
    "\n",
    "    # Get feature names from preprocessor\n",
    "    num_feature_names = list(best_model.named_steps['preprocessor'].transformers_[0][2])\n",
    "    cat_feature_names = list(best_model.named_steps['preprocessor'].transformers_[1][1]['onehot'].get_feature_names_out(category_encoded_columns))\n",
    "\n",
    "    # Combine numerical and categorical feature names\n",
    "    feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "    # Print feature importances\n",
    "    print(\"\\nFeature Importances using Gain Ratio Attribute Evaluator:\")\n",
    "    for name, importance in zip(feature_names, feature_importances):\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"Base estimator does not support feature importances.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
