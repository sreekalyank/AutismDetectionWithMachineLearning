{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91475b5-a921-4309-a028-3931bf62db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons\n",
      "0      0   0   0   0   0   0   1   1   0    1        28\n",
      "1      1   1   0   0   0   1   1   0   0    0        36\n",
      "2      1   0   0   0   0   0   1   1   0    1        36\n",
      "3      1   1   1   1   1   1   1   1   1    1        24\n",
      "4      1   1   0   1   1   1   1   1   1    1        20\n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...       ...\n",
      "1049   0   0   0   0   0   0   0   0   0    1        24\n",
      "1050   0   0   1   1   1   0   1   0   1    0        12\n",
      "1051   1   0   1   1   1   1   1   1   1    1        18\n",
      "1052   1   0   0   0   0   0   0   1   0    1        19\n",
      "1053   1   1   0   0   1   1   0   1   1    0        24\n",
      "\n",
      "[1054 rows x 11 columns]\n",
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "print(features)\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_ab=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d72f436-3d2f-4251-9e6a-f382bf742213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 0.96470588 0.94117647 0.94047619 0.9047619  0.95238095\n",
      " 0.94047619 0.91666667 0.91666667 0.96428571]\n",
      "Mean CV accuracy: 0.939453781512605\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "ROC AUC: 0.9994896917738314\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.12395449227564767\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_qt_rf=test_accuracy_test\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e0b604-c263-48de-bec5-e32d24ef83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.87058824 0.92941176 0.95294118 0.92857143 0.95238095 0.91666667\n",
      " 0.89285714 0.9047619  0.9047619  0.86904762]\n",
      "Mean CV accuracy: 0.9121988795518208\n",
      "Testing Set Accuracy  : 0.966824644549763\n",
      "Testing Set Accuracy  : 0.966824644549763\n",
      "printing precision\n",
      "0.9580835331734612\n",
      "f1-score\n",
      "0.9627217889503041\n",
      "ROC AUC: 0.9679016125739947\n",
      "recall\n",
      "0.9679016125739948\n",
      "kappa score\n",
      "0.9254605097148625\n",
      "log loss\n",
      "1.1957610129090999\n",
      "MCC\n",
      "0.9259330944845972\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=None, min_samples_split=2,random_state=35)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=100, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_dt=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e9ba4fc-a0c1-44f0-a506-3979a82f34aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 0.90588235 0.94117647 0.91666667 0.94047619 0.92857143\n",
      " 0.92857143 0.92857143 0.95238095 0.92857143]\n",
      "Mean CV accuracy: 0.9323809523809524\n",
      "Testing Set Accuracy  : 0.9715639810426541\n",
      "Testing Set Accuracy  : 0.9715639810426541\n",
      "printing precision\n",
      "0.96\n",
      "f1-score\n",
      "0.9683752997601918\n",
      "ROC AUC: 0.997040212288222\n",
      "recall\n",
      "0.9788732394366197\n",
      "kappa score\n",
      "0.9368074273734651\n",
      "log loss\n",
      "0.13239564713205948\n",
      "MCC\n",
      "0.9386835252434019\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=11, weights='distance',algorithm='auto', p=2, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=100,subsample=500,random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_knn=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_cv.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_cv.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_cv.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70700ded-f4e2-45c7-96e5-a12bb6e97cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 2.154434690031882}\n",
      "Best Score: 0.9535834266517359\n",
      "Testing Set Accuracy: 0.9811320754716981\n",
      "Precision: 0.967741935483871\n",
      "F1-score: 0.9767543859649122\n",
      "ROC AUC: 0.9977608598298253\n",
      "Recall: 0.987012987012987\n",
      "Kappa Score: 0.9535291538798772\n",
      "Log Loss: 0.16175233379085582\n",
      "Matthews Correlation Coefficient: 0.9545604164247246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=300, random_state=69))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(5,-9, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_qt_gnb=test_accuracy\n",
    "\n",
    "# Additional imports\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "#QT_GNB_Toddlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e8bbd33-51b7-4597-a542-f287cc5751fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98611111 0.98611111 0.98611111 1.         0.98611111 0.98611111\n",
      " 1.         0.98591549 1.         1.        ]\n",
      "Mean CV accuracy: 0.9916471048513301\n",
      "Testing Set Accuracy  : 0.9881656804733728\n",
      "printing precision\n",
      "0.9861843449826282\n",
      "f1-score\n",
      "0.9861843449826282\n",
      "ROC AUC: 0.9997138769670959\n",
      "recall\n",
      "0.9861843449826282\n",
      "kappa score\n",
      "0.9723686899652565\n",
      "log loss\n",
      "0.060630595062107835\n",
      "MCC\n",
      "0.9723686899652565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.32, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_lr=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#QT_LR_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69c4fbc-946c-420e-9487-617f1e72d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97849462 1.         0.97849462 0.98924731 0.96774194 0.96774194\n",
      " 0.98924731 1.         0.93478261 0.97826087]\n",
      "Mean CV accuracy: 0.9784011220196354\n",
      "Testing Set Accuracy  : 0.984251968503937\n",
      "printing precision\n",
      "0.9887640449438202\n",
      "f1-score\n",
      "0.981497668997669\n",
      "ROC AUC: 0.9997126436781609\n",
      "recall\n",
      "0.975\n",
      "kappa score\n",
      "0.9630061170987474\n",
      "log loss\n",
      "0.027607827447274497\n",
      "MCC\n",
      "0.9636657539796973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.12, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_svm=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#QT_SVM_Toddlers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4a563e-afb1-43eb-a428-5a6e5b425a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92631579 0.96842105 0.92631579 0.94736842 0.96842105 0.94736842\n",
      " 0.91578947 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9472788353863383\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "ROC AUC: 0.9991043439319302\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "log loss\n",
      "0.06658456169079688\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('quantile', QuantileTransformer(n_quantiles=10, random_state=42))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_qt_lda=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#QT_LDA_Toddlers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4aacd0b-2b3f-4c9d-9a9c-1dd6655ee021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_ab=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d06cefa-6edd-45a4-a4cf-040be574d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 0.96470588 0.94117647 0.94047619 0.9047619  0.95238095\n",
      " 0.94047619 0.91666667 0.91666667 0.96428571]\n",
      "Mean CV accuracy: 0.939453781512605\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "ROC AUC: 0.9994896917738313\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.12435094226362926\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_rf=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8531ce7-2bcc-4650-9577-214075eec5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.89873418 0.89873418 0.89873418 0.91139241 0.94936709 0.93670886\n",
      " 0.89873418 0.84810127 0.89873418 0.91139241]\n",
      "Mean CV accuracy: 0.9050632911392403\n",
      "Testing Set Accuracy  : 0.9507575757575758\n",
      "Testing Set Accuracy  : 0.9507575757575758\n",
      "printing precision\n",
      "0.9369764270407169\n",
      "f1-score\n",
      "0.9434326119562888\n",
      "Testing Set Accuracy without cross-validation: 0.9507575757575758\n",
      "Testing Set Accuracy without cross-validation: 0.9507575757575758\n",
      "ROC AUC: 0.9508844813722862\n",
      "recall\n",
      "0.9508844813722862\n",
      "kappa score\n",
      "0.8869118228548833\n",
      "log loss\n",
      "1.7748768714337992\n",
      "MCC\n",
      "0.8877519691404088\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy',splitter='random',max_depth=None, min_samples_split=2,random_state=35)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_dt=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12351439-5d25-47ee-9612-0f3ad2cb0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96385542 0.97590361 0.91463415 0.96341463 0.96341463 0.96341463\n",
      " 0.91463415 0.97560976 0.92682927 0.92682927]\n",
      "Mean CV accuracy: 0.9488539523949457\n",
      "Testing Set Accuracy  : 0.9827586206896551\n",
      "Testing Set Accuracy  : 0.9827586206896551\n",
      "printing precision\n",
      "0.974025974025974\n",
      "f1-score\n",
      "0.9802972399150742\n",
      "Testing Set Accuracy  : 0.9827586206896551\n",
      "Testing Set Accuracy without cross-validation: 0.9827586206896551\n",
      "ROC AUC: 0.9997415352804343\n",
      "recall\n",
      "0.9874213836477987\n",
      "kappa score\n",
      "0.9606078614483403\n",
      "log loss\n",
      "0.09197860141703099\n",
      "MCC\n",
      "0.96135403706384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=11, weights='distance', algorithm='auto', p=2, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline with PowerTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transform', PowerTransformer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_knn=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c275eab-0807-4d17-aaec-85364c09f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 2.154434690031884}\n",
      "Best Score: 0.9655811403508773\n",
      "Testing Set Accuracy: 0.9894736842105263\n",
      "Precision: 0.9827586206896552\n",
      "F1-score: 0.9874686716791979\n",
      "ROC AUC: 1.0\n",
      "Recall: 0.9925373134328358\n",
      "Kappa Score: 0.9749406489053021\n",
      "Log Loss: 0.15181935596283097\n",
      "Matthews Correlation Coefficient: 0.975246910420175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(1,-1, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=35)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_pt_gnb=test_accuracy_cv\n",
    "# Evaluate other metrics\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "\n",
    "#PT_GNB_Toddlers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d75c57-04a7-4b3d-994e-7cf09a012ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_validation.py:839: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 136, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_response.py\", line 194, in _get_response_values\n",
      "    y_pred = prediction_method(X)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\pipeline.py\", line 514, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 827, in transform\n",
      "    Xs = self._fit_transform(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 681, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\pipeline.py\", line 940, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\pipeline.py\", line 696, in transform\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1027, in transform\n",
      "    X_int, X_mask = self._transform(\n",
      "  File \"C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 200, in _transform\n",
      "    raise ValueError(msg)\n",
      "ValueError: Found unknown categories [1.0] in column 4 during transform\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [ 1. nan  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Mean CV accuracy: nan\n",
      "Testing Set Accuracy  : 0.9981024667931688\n",
      "printing precision\n",
      "0.9986072423398329\n",
      "f1-score\n",
      "0.9978189704050424\n",
      "ROC AUC: 0.999983471620773\n",
      "recall\n",
      "0.9970414201183432\n",
      "kappa score\n",
      "0.995637958862724\n",
      "log loss\n",
      "0.037436289606112505\n",
      "MCC\n",
      "0.9956474312001843\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power_transformer', PowerTransformer())  # Change Normalizer to PowerTransformer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=69))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_lr=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#PT_LR_Toddlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5b6a49-e7f9-4452-8e45-a06ec57511c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.98947368 1.         0.98947368 0.96842105 0.98947368\n",
      " 0.98947368 0.98947368 1.         0.9893617 ]\n",
      "Mean CV accuracy: 0.990515117581187\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.9873417721518987\n",
      "f1-score\n",
      "0.9757326007326008\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9655172413793103\n",
      "kappa score\n",
      "0.951487414187643\n",
      "log loss\n",
      "0.021129956772216775\n",
      "MCC\n",
      "0.9526090433773056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))  # Use PowerTransformer instead of QuantileTransformer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True))  # Support Vector Classifier\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_svm=test_accuracy_cv\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True))  # Support Vector Classifier\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b5cef7-4ab0-4e6d-b965-d2a42b84d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92631579 0.96842105 0.92631579 0.94736842 0.96842105 0.94736842\n",
      " 0.91578947 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9472788353863383\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "ROC AUC: 0.9995521719659651\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "log loss\n",
      "0.06566255495861267\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('power', PowerTransformer(method='yeo-johnson', standardize=True))  # Use PowerTransformer instead of QuantileTransformer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_pt_lda=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#PT_LDA_Toddler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0919ec-63cc-4768-9c2d-f788048bf27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__learning_rate': 1.0, 'classifier__n_estimators': 150}\n",
      "Testing Set Accuracy: 0.9900497512437811\n",
      "printing precision\n",
      "0.9885378649635037\n",
      "f1-score\n",
      "0.9885378649635037\n",
      "Testing Set Accuracy  : 0.9701492537313433\n",
      "Testing Set Accuracy without cross-validation: 0.9701492537313433\n",
      "ROC AUC: 0.9989735401459854\n",
      "recall\n",
      "0.9781021897810219\n",
      "kappa score\n",
      "0.9329030822298876\n",
      "log loss\n",
      "0.0891170617597529\n",
      "MCC\n",
      "0.9350101626030657\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.19, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),\n",
    "                               n_estimators=100, learning_rate=0.5, algorithm='SAMME.R', random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__learning_rate': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = grid_search.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_nor_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bfc6fc2-434d-45c5-ad1f-175ce258d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Random Forest): {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "Testing Set Accuracy (Random Forest): 0.9835390946502057\n",
      "printing precision\n",
      "0.9842836257309941\n",
      "f1-score\n",
      "0.9804190169218372\n",
      "Testing Set Accuracy  : 0.9835390946502057\n",
      "Testing Set Accuracy without cross-validation: 0.9835390946502057\n",
      "ROC AUC: 0.9987206141052295\n",
      "recall\n",
      "0.9767711498480729\n",
      "kappa score\n",
      "0.9608411892675852\n",
      "log loss\n",
      "0.09137991788687001\n",
      "MCC\n",
      "0.9610254129675445\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.23, random_state=42)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', random_forest)\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters (Random Forest):\", grid_search_rf.best_params_)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test_rf = grid_search_rf.predict(X_test)\n",
    "test_accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(\"Testing Set Accuracy (Random Forest):\", test_accuracy_test_rf)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test_rf, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test_rf,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_nor_rf=test_accuracy_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test_rf,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test_rf))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "754cd947-c16d-49ed-9b0a-440dc8079d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92771084 0.95180723 0.85365854 0.95121951 0.90243902 0.95121951\n",
      " 0.86585366 0.92682927 0.95121951 0.93902439]\n",
      "Mean CV accuracy: 0.9220981486923303\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "printing precision\n",
      "0.9700180925303696\n",
      "f1-score\n",
      "0.9700180925303696\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "Testing Set Accuracy  : 0.9741379310344828\n",
      "ROC AUC: 0.9700180925303697\n",
      "recall\n",
      "0.9700180925303696\n",
      "kappa score\n",
      "0.9400361850607392\n",
      "log loss\n",
      "0.9321634497185471\n",
      "MCC\n",
      "0.9400361850607392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='log_loss',splitter='random',max_depth=None, min_samples_split=2,random_state=35)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_dt=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "861cd12d-3dfd-4194-b41e-2527e4c3efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92708333 0.89583333 0.90625    0.91666667 0.86458333 0.88541667\n",
      " 0.875      0.9375     0.89583333 0.85263158]\n",
      "Mean CV accuracy: 0.8956798245614035\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "printing precision\n",
      "0.9827586206896552\n",
      "f1-score\n",
      "0.9874686716791979\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "Testing Set Accuracy  : 0.9894736842105263\n",
      "ROC AUC: 0.9973347547974414\n",
      "recall\n",
      "0.9925373134328358\n",
      "kappa score\n",
      "0.9749406489053021\n",
      "log loss\n",
      "0.13411050267706187\n",
      "MCC\n",
      "0.975246910420175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=11, weights='distance', algorithm='auto', p=2, n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline with Normalizer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_nor_knn=test_accuracy_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5761fb13-fb4b-4c67-af98-5b84c6afe6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 0.21544346900318845}\n",
      "Best Score: 0.9209182530795073\n",
      "Testing Set Accuracy: 0.9716981132075472\n",
      "Precision: 0.953125\n",
      "F1-score: 0.9654760612311366\n",
      "ROC AUC: 0.9914912673533363\n",
      "Recall: 0.9805194805194806\n",
      "Kappa Score: 0.9310195227765726\n",
      "Log Loss: 0.1067825093691721\n",
      "Matthews Correlation Coefficient: 0.9332424971257783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(1,-2, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_nor_gnb=test_accuracy_cv\n",
    "# Evaluate other metrics\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "#Norma_GNB_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0a358d0-bea8-4a45-b098-e7508de6d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.     0.975  0.9875 0.975  1.     1.     1.     0.9875 0.9875 0.9875]\n",
      "Mean CV accuracy: 0.99\n",
      "Testing Set Accuracy  : 0.9881422924901185\n",
      "printing precision\n",
      "0.9817073170731707\n",
      "f1-score\n",
      "0.986335403726708\n",
      "ROC AUC: 0.999418012512731\n",
      "recall\n",
      "0.9913793103448276\n",
      "kappa score\n",
      "0.9726752349065774\n",
      "log loss\n",
      "0.08585858958863242\n",
      "MCC\n",
      "0.9730385588484597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.24, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())  # Change QuantileTransformer to Normalizer\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_lr=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#Norma_LR_Toddler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9993e06-0a18-4e29-abab-961c37c96381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97916667 0.96875    1.         0.92708333 0.96875    0.98958333\n",
      " 0.96875    0.94791667 0.96875    0.94736842]\n",
      "Mean CV accuracy: 0.9666118421052632\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "printing precision\n",
      "0.9855072463768115\n",
      "f1-score\n",
      "0.974128540305011\n",
      "ROC AUC: 1.0\n",
      "recall\n",
      "0.9642857142857143\n",
      "kappa score\n",
      "0.9482852476864453\n",
      "log loss\n",
      "0.06471067908005945\n",
      "MCC\n",
      "0.949555851279846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Preprocessing pipeliney\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_svm=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.decision_function(X_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))  \n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "##Norma_SVM_Toddler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "512b3232-136f-40b0-8c13-2dc6d55f03cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.91578947 0.95789474 0.91578947 0.94736842 0.96842105 0.93684211\n",
      " 0.90526316 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9420156774916013\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('normalizer', Normalizer())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LDA())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_nor_lda=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LDA())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "##Norma_LDA_Toddler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dce8c45-4fed-4f47-825b-6f90be974df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95294118 1.         0.97647059 0.98809524 1.         0.98809524\n",
      " 0.97619048 0.96428571 0.98809524 0.96428571]\n",
      "Mean CV accuracy: 0.9798459383753502\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "printing precision\n",
      "0.9892324964278425\n",
      "f1-score\n",
      "0.9892324964278425\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "Testing Set Accuracy  : 0.990521327014218\n",
      "ROC AUC: 0.9997958767095325\n",
      "recall\n",
      "0.9892324964278425\n",
      "kappa score\n",
      "0.9784649928556848\n",
      "log loss\n",
      "0.3636137540542667\n",
      "MCC\n",
      "0.9784649928556848\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define AdaBoost classifier with a decision tree base estimator\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=2),n_estimators=64,learning_rate=0.5,algorithm='SAMME.R',random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mas', MaxAbsScaler())  # Replace 'quantile' with 'mas'\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and AdaBoost classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ada_boost)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_mas_ab=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01780e21-1bdb-423b-973d-2b7355d60501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 50}\n",
      "Testing Set Accuracy with best hyperparameters: 0.9827586206896551\n",
      "printing precision\n",
      "0.980012061686913\n",
      "f1-score\n",
      "0.980012061686913\n",
      "Testing Set Accuracy  : 0.9913793103448276\n",
      "Testing Set Accuracy without cross-validation: 0.9913793103448276\n",
      "ROC AUC: 0.9998276901869562\n",
      "recall\n",
      "0.9900060308434565\n",
      "kappa score\n",
      "0.980012061686913\n",
      "log loss\n",
      "0.3632383522980847\n",
      "MCC\n",
      "0.980012061686913\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.22, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline with MaxAbsScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Random Forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=35))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [10, 15, 20],\n",
    "    \n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from the grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the best model on the separate testing set\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "test_accuracy_test_best = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy with best hyperparameters:\", test_accuracy_test_best)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "accuracy_mas_rf=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaf38171-e820-4d5d-9214-520d9f3ebc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96875    0.91666667 0.88541667 0.96875    0.91666667 0.9375\n",
      " 0.91666667 0.91666667 0.85416667 0.91578947]\n",
      "Mean CV accuracy: 0.919703947368421\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "printing precision\n",
      "0.9746801705756929\n",
      "f1-score\n",
      "0.9746801705756929\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "ROC AUC: 0.974680170575693\n",
      "recall\n",
      "0.9746801705756929\n",
      "kappa score\n",
      "0.9493603411513859\n",
      "log loss\n",
      "0.7588137555603612\n",
      "MCC\n",
      "0.9493603411513859\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Define Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=None, min_samples_split=2, random_state=35)\n",
    "\n",
    "# Preprocessing pipeline with MaxAbsScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "\n",
    "# Create pipeline with preprocessing and Decision Tree classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', decision_tree)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_mas_dt=test_accuracy_test\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c60d395-38a2-4d9c-aef5-02d4bdcc06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross-validation scores: [0.9375     0.94791667 0.88541667 0.96875    0.86458333 0.91666667\n",
      " 0.91666667 0.92708333 0.875      0.90526316]\n",
      "Mean KNN CV accuracy: 0.9144846491228069\n",
      "Testing Set Accuracy with KNN (cross-validation): 0.9789473684210527\n",
      "printing precision\n",
      "0.9666666666666667\n",
      "f1-score\n",
      "0.9751828631138977\n",
      "Testing Set Accuracy  : 0.9789473684210527\n",
      "Testing Set Accuracy without cross-validation: 0.9789473684210527\n",
      "ROC AUC: 0.974680170575693\n",
      "recall\n",
      "0.9746801705756929\n",
      "kappa score\n",
      "0.9493603411513859\n",
      "log loss\n",
      "0.7588137555603612\n",
      "MCC\n",
      "0.9493603411513859\n"
     ]
    }
   ],
   "source": [
    "#### import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.09, random_state=42)\n",
    "\n",
    "# Define KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', n_jobs=3)\n",
    "\n",
    "# Preprocessing pipeline with MaxAbsScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and KNN classifier for training with cross-validation\n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "# Perform cross-validation on training set using KNN\n",
    "cv_scores_knn = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"KNN Cross-validation scores:\", cv_scores_knn)\n",
    "print(\"Mean KNN CV accuracy:\", np.mean(cv_scores_knn))\n",
    "\n",
    "# Train the model on the entire training set using KNN\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy with KNN (cross-validation):\", test_accuracy_cv)\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, roc_curve, auc, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "print(\"printing precision\")\n",
    "precision=precision_score(y_test, y_pred_cv, average='macro')\n",
    "print(precision)\n",
    "print(\"f1-score\")\n",
    "\n",
    "# # F1 Score\n",
    "print(f1_score(y_test, y_pred_cv,average='macro'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_test)\n",
    "accuracy_mas_knn=test_accuracy_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_prob_test = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy without cross-validation:\", test_accuracy_test)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "# # Recall\n",
    "print(recall_score(y_test, y_pred_test,average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "# # Kappa Score\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "# # Log Loss\n",
    "print(log_loss(y_test, pipeline_test.predict_proba(X_test)))\n",
    "\n",
    "print('MCC')\n",
    "# # Matthews Correlation Coefficient\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf885ecd-98dd-487c-87fc-bcca50adce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__var_smoothing': 2.154434690031884}\n",
      "Best Score: 0.9578163493840985\n",
      "Testing Set Accuracy: 0.9716981132075472\n",
      "Precision: 0.9600877192982455\n",
      "F1-score: 0.9647723496178133\n",
      "ROC AUC: 0.9986565158978953\n",
      "Recall: 0.9697716077026421\n",
      "Kappa Score: 0.9295525033229951\n",
      "Log Loss: 0.1818012071681241\n",
      "Matthews Correlation Coefficient: 0.9298088998906792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('max_abs_scaler', MaxAbsScaler())  # Replace Normalizer with MaxAbsScaler\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Gaussian Naive Bayes classifier for training\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': np.logspace(1,-2, num=10) # Varying var_smoothing parameter\n",
    "}\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "accuracy_mas_gnb=test_accuracy\n",
    "# Evaluate other metrics\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob_test = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Kappa score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "print(\"Kappa Score:\", kappa_score)\n",
    "\n",
    "# Calculate log loss\n",
    "logloss = log_loss(y_test, best_model.predict_proba(X_test))\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "#Norma_GNB_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05b626a4-56b4-4d9b-bb26-e8d2cedc043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98611111 0.98611111 0.98611111 1.         0.98611111 0.98611111\n",
      " 1.         0.98591549 1.         1.        ]\n",
      "Mean CV accuracy: 0.9916471048513301\n",
      "Testing Set Accuracy  : 0.9881656804733728\n",
      "printing precision\n",
      "0.9861843449826282\n",
      "f1-score\n",
      "0.9861843449826282\n",
      "ROC AUC: 0.9996730022481095\n",
      "recall\n",
      "0.9861843449826282\n",
      "kappa score\n",
      "0.9723686899652565\n",
      "log loss\n",
      "0.060362802581865146\n",
      "MCC\n",
      "0.9723686899652565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.32, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_mas_lr=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and Logistic Regression classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#MSA_LR_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84619f50-dc73-4d40-9108-8149b2a6f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97849462 1.         0.97849462 0.98924731 0.96774194 0.96774194\n",
      " 1.         1.         0.94565217 0.97826087]\n",
      "Mean CV accuracy: 0.9805633473585788\n",
      "Testing Set Accuracy  : 0.984251968503937\n",
      "printing precision\n",
      "0.9887640449438202\n",
      "f1-score\n",
      "0.981497668997669\n",
      "ROC AUC: 0.9994252873563217\n",
      "recall\n",
      "0.975\n",
      "kappa score\n",
      "0.9630061170987474\n",
      "log loss\n",
      "0.026425866315960663\n",
      "MCC\n",
      "0.9636657539796973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.12, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and SVM classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_mas_svm=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and SVM classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))  # Setting probability=True to enable predict_proba\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#MSA_SVM_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bcc5f4d-67c6-49ec-ba0f-34df4e247399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.92631579 0.96842105 0.92631579 0.94736842 0.97894737 0.94736842\n",
      " 0.92631579 0.95789474 0.93617021 0.9787234 ]\n",
      "Mean CV accuracy: 0.9493840985442329\n",
      "Testing Set Accuracy  : 0.9811320754716981\n",
      "printing precision\n",
      "0.967741935483871\n",
      "f1-score\n",
      "0.9767543859649122\n",
      "ROC AUC: 0.9995521719659651\n",
      "recall\n",
      "0.987012987012987\n",
      "kappa score\n",
      "0.9535291538798772\n",
      "log loss\n",
      "0.06453460393260697\n",
      "MCC\n",
      "0.9545604164247246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score, cohen_kappa_score, log_loss, matthews_corrcoef\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\shanm\\\\OneDrive\\\\Desktop\\\\archive\\\\Toddler Autism dataset July 2018.csv\")\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "features = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1]\n",
    "\n",
    "# Select categorical features\n",
    "category_features = features.iloc[:, [12, 13, 14, 15, 16]]\n",
    "\n",
    "# Drop categorical columns\n",
    "features.drop(features.columns[-6:], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# One Hot Encoding without changing column names\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "category_transformed = enc.fit_transform(category_features)\n",
    "category_encoded_columns = enc.get_feature_names_out(category_features.columns)\n",
    "transformed_df = pd.DataFrame(category_transformed, columns=category_encoded_columns)\n",
    "\n",
    "# Concatenate encoded features with numerical features\n",
    "result_df = pd.concat([features, transformed_df], axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(result_df, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', MaxAbsScaler())\n",
    "        ]), ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons']),\n",
    "        ('cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder())\n",
    "        ]), transformed_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns without transformation\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and LDA classifier for training  \n",
    "pipeline_cv = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training set\n",
    "cv_scores = cross_val_score(pipeline_cv, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_cv = pipeline_cv.predict(X_test)\n",
    "test_accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "print(\"Testing Set Accuracy  :\", test_accuracy_cv)\n",
    "accuracy_mas_lda=test_accuracy_cv\n",
    "# Create pipeline with preprocessing and LDA classifier for testing without cross-validation\n",
    "pipeline_test = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Train the model on the entire training set without cross-validation\n",
    "pipeline_test.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_test = pipeline_test.predict(X_test)\n",
    "test_accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"printing precision\")\n",
    "print(precision_score(y_test, y_pred_test, average='macro'))\n",
    "print(\"f1-score\")\n",
    "print(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "# Test the model on the separate testing set\n",
    "y_pred_prob = pipeline_test.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "print('recall')\n",
    "print(recall_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "print('kappa score')\n",
    "print(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "print('log loss')\n",
    "print(log_loss(y_test, y_pred_prob))\n",
    "\n",
    "print('MCC')\n",
    "print(matthews_corrcoef(y_test, y_pred_test))\n",
    "\n",
    "#MSA_LDA_Toddlers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e61aba2d-afc6-402d-a534-2c5c548c160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "transformers = ['qt', 'pt','nor','mas']\n",
    "models = ['ab', 'rf', 'dt', 'knn', 'gnb', 'lr', 'svm', 'lda']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collecting accuracies for each transformer\n",
    "accuracies = {}\n",
    "for transformer in transformers:\n",
    "    accuracies[transformer] = [globals()[f\"accuracy_{transformer}_{model}\"] for model in models]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for transformer in transformers:\n",
    "    ax.plot(models, accuracies[transformer], label=transformer)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of Models for Different Transformers')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
